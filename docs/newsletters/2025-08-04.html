<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qwenはすべてを解決します | AIニュース</title>
    <meta name="description" content="Qwenはすべてを解決します - AIニュース 2025-08-04。最新のAI技術動向を日本語でお届け。">
    <meta name="keywords" content="AI,人工知能,ニュースレター,2025-08-04,機械学習,深層学習,日本語">
    <meta name="author" content="AIニュース">
    <link rel="canonical" href="https://yipg.github.io/ainews/docs/newsletters/2025-08-04.html">
    
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="Qwenはすべてを解決します | AIニュース">
    <meta property="og:description" content="Qwenはすべてを解決します - AIニュース 2025-08-04。最新のAI技術動向を日本語でお届け。">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yipg.github.io/ainews/docs/newsletters/2025-08-04.html">
    <meta property="og:image" content="https://yipg.github.io/ainews/newsletters/og/2025-08-04.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:type" content="image/png">
    <meta property="og:site_name" content="AIニュース">
    <meta property="og:locale" content="ja_JP">
    <meta property="article:published_time" content="2025-08-04T09:00:00+00:00">
    <meta property="article:author" content="AIニュース">
    <meta property="article:section" content="AI技術ニュース">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Qwenはすべてを解決します | AIニュース">
    <meta name="twitter:description" content="Qwenはすべてを解決します - AIニュース 2025-08-04。最新のAI技術動向を日本語でお届け。">
    <meta name="twitter:image" content="https://yipg.github.io/ainews/newsletters/og/2025-08-04.png">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>✏️</text></svg>">
    <link rel="alternate icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA">
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="AIニュース RSS Feed" href="../feed.xml">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Verdana, Geneva, sans-serif;
            font-size: 1em;
            line-height: 1.7;
            letter-spacing: 0.02em;
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            color: #111;
            word-wrap: break-word;
        }
        
        nav {
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: nowrap;
        }
        
        .site-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #111;
            text-decoration: none;
            flex-shrink: 0;
        }
        
        .site-title:hover {
            text-decoration: underline;
        }
        
        .nav-links {
            font-size: 0.85em;
            white-space: nowrap;
        }
        
        .nav-links a {
            color: #111;
            text-decoration: none;
            margin-left: 12px;
        }
        
        .nav-links a:hover {
            text-decoration: underline;
        }
        
        
        h1, h2, h3, h4, h5, h6 {
            margin: 35px 0 20px 0;
            line-height: 1.3;
            color: #111;
            letter-spacing: 0.01em;
        }
        
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.3em; }
        h3 { font-size: 1.1em; }
        
        p {
            margin: 20px 0;
        }
        
        a {
            color: #0969da;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        blockquote {
            border-left: 3px solid #ccc;
            margin: 25px 0;
            padding: 0 25px;
            color: #555;
            font-style: italic;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 25px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 25px 0;
            border-radius: 3px;
        }
        
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 35px 0;
        }
        
        footer {
            margin-top: 40px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #555;
            font-size: 0.85em;
        }
        
        footer a {
            color: #555;
            text-decoration: none;
            margin: 0 8px;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 15px;
                font-size: 0.95em;
            }
            
            nav {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            
            .nav-links {
                font-size: 0.8em;
            }
            
            .nav-links a {
                margin-left: 0;
                margin-right: 12px;
            }
            
            .article-title {
                font-size: 1.4em;
            }
        }
        
        @media (max-width: 480px) {
            body {
                font-size: 0.9em;
                padding: 12px;
            }
            
            .site-title {
                font-size: 1em;
            }
            
            .nav-links {
                font-size: 0.75em;
            }
            
            .article-title {
                font-size: 1.3em;
            }
        }
        
        @media (prefers-color-scheme: dark) {
            body {
                background-color: #111;
                color: #eee;
            }
            
            nav {
                border-bottom-color: #444;
            }
            
            .site-title, .nav-links a, .article-title, h1, h2, h3, h4, h5, h6 {
                color: #eee;
            }
            
            .article-date {
                color: #ccc;
            }
            
            blockquote {
                border-left-color: #555;
                color: #ccc;
            }
            
            code {
                background-color: #2d3748;
                color: #e2e8f0;
            }
            
            pre {
                background-color: #2d3748;
            }
            
            hr, footer {
                border-color: #444;
            }
            
            footer, footer a {
                color: #ccc;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html" class="site-title">✏️ AIニュース</a>
        <div class="nav-links">
            <a href="../index.html">ホーム</a>
            <a href="./archive.html">アーカイブ</a>
            <a href="../feed.xml">RSS</a>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
        </div>
    </nav>

    <main>
        <h1 id="qwen">Qwenはすべてを解決します</h1>
<p>AlibabaのQwenチームが驚きのモデルを公開しました。20B MMDiTモデルは「ネイティブテキストを含む驚くべきグラフィックポスターの作成に特に強い」と発表されています。詳細は以下のリンクをご覧ください：<a href="https://qwenlm.github.io/blog/qwen-image/">ブログ</a>、<a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf#page=14.57">論文</a>。</p>
<p>このモデルは、英語だけでなく中国語でも優れたテキストレンダリングを実現しており、画像生成だけでなく画像編集にも驚くべき性能を発揮します。他のモデルとの比較では、<a href="https://flux-ai.io/flux-kontext/">Flux Kontext</a>に匹敵する性能を示しています。</p>
<p>技術レポート（46ページ）は、西洋の研究所では珍しい透明性を示しており、合成データを使用してテキストレンダリング結果を達成する方法について一部の洞察を提供しています。</p>
<hr />
<h1 id="ai-twitter-recap">AI Twitter Recap</h1>
<p><strong>フロンティア推論：Gemini 2.5 Deep Think、新しい数学/証明システム、モデル間評価</strong></p>
<ul>
<li>
<p><strong>GoogleのGemini 2.5 Deep ThinkがUltraサブスクライバー向けに提供開始</strong><br />
  DeepMindは難しいベンチマークでSOTAを達成したと主張しており、初期ユーザーは以前のGeminiと比較して大幅な性能向上を報告しています。一部のタスクではOpenAIのo3 Proに近い性能を示しています。コミュニティテストによる定量的な差分は以下の通りです：AIME（2025年）+11.2%、HLE（知識）+13.2%、LiveCodeBench（コーディング）+13.4%。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/swyx/status/1951460518293807241">@swyx</a>、<a href="https://twitter.com/demishassabis/status/1951468051578142848">@demishassabis</a>、<a href="https://twitter.com/tulseedoshi/status/1952059171727437859">@tulseedoshi</a>、<a href="https://twitter.com/MParakhin/status/1952028947153371631">@MParakhin</a>。</p>
</li>
<li>
<p><strong>数学と定理証明の進展</strong>  </p>
</li>
<li>ByteDanceのSeedProverはPutnamBenchで331/657（以前のSOTAの約4倍）、軽量推論で201/657、OpenAIのminiF2Fで100%を達成し、DeepMindのAlphaGeometry2を上回りました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/teortaxesTex/status/1951875052967739787">@teortaxesTex</a>、<a href="https://twitter.com/cgeorgiaw/status/1952301113446699347">@cgeorgiaw</a>、<a href="https://twitter.com/Dorialexander/status/1952094475725238479">@Dorialexander</a>。  </li>
<li>OpenAIは「ユニバーサル検証器」を開発中で、数学/コーディングの成果を主観的な領域に転送することを目指しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/steph_palazzolo/status/1952375778361954801">@steph_palazzolo</a>、<a href="https://twitter.com/corbtt/status/1952437149544144984">@corbtt</a>。  </li>
<li>
<p>「Hieroglyph」ベンチマークは横方向の推論を調査します（Only Connectスタイル）。モデルは最も難しい20問で50%未満のスコアを記録しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/synthwavedd/status/1951645151203324099">@synthwavedd</a>。</p>
</li>
<li>
<p><strong>ベンチマークのメタシフト</strong><br />
  推論モデルは推論に適したタスクで計算能力の10倍相当の向上を示しており、これは元のTransformerジャンプに匹敵します。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/EpochAIResearch/status/1951734757483487450">@EpochAIResearch</a>。KaggleとGoogleは競技ゲーム（テキストチェスから開始）でモデルをテストするGame Arenaを立ち上げました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/GoogleDeepMind/status/1952406075996533077">@GoogleDeepMind</a>、<a href="https://twitter.com/demishassabis/status/1952436066524299432">@demishassabis</a>。Artificial Analysisはモデルインデックスを更新し、IFBench（指示追従）を追加しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/ArtificialAnlys/status/1952302030812483982">thread</a>。</p>
</li>
</ul>
<hr />
<p><strong>オープンウェイトモデルの波：Qwen-Image、GLM-4.5の勢い、XBai o4、Tencent Hunyuan、効率的なトレーニング</strong></p>
<ul>
<li>
<p><strong>Qwen-Image（20B MMDiT）がApache-2.0でリリース</strong><br />
  強力なバイリンガルテキストレンダリング（英語でGPT-4oに匹敵、中国語で最高クラス）を実現し、ピクセル内テキスト合成や幅広い画像スタイルをサポート。オープンウェイト、コード、デモは以下のリンクをご覧ください：<a href="https://twitter.com/Alibaba_Qwen/status/1952398250121756992">Twitter</a>。コミュニティは、微調整されたWan 2.1 VAEとQwen VLテキストエンコーダーコンポーネントを活用していると指摘しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/multimodalart/status/1952409238413684901">Twitter</a>、<a href="https://twitter.com/victormustar/status/1952416615351366033">@victormustar</a>。</p>
</li>
<li>
<p><strong>Zhipu AIのGLM-4.5がリーダーボードで上昇</strong><br />
  LMSYS Arenaで総合5位にランクインし、4K以上の投票を獲得。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/lmarena_ai/status/1952402506497020330">Twitter</a>、<a href="https://twitter.com/Zai_org/status/1952404744225349799">@Zai_org</a>。Terminal-Benchは、推論/コードアシスタント間でトップクラスの性能を確認しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/Zai_org/status/1952411485742760324">Twitter</a>。</p>
</li>
<li>
<p><strong>XBai o4（並列テスト時スケーリング）</strong><br />
  Apache-2.0でオープンウェイトを提供。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/theMetaStoneAI/status/1951486506562101656">Twitter</a>。</p>
</li>
<li>
<p><strong>Tencent Hunyuan小型ファミリー（0.5B/1.8B/4B/7B）</strong><br />
  エッジ対応モデル（シングルカード展開）で256Kコンテキスト、ツール/エージェントスキル、多フレームワークサポート（SGLang、vLLM、TensorRT-LLM）を提供。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/TencentHunyuan/status/1952262079051940322">Twitter</a>。</p>
</li>
<li>
<p><strong>大規模でありながら手頃なVLMs</strong><br />
  StepFunのStep-3（321B MoE）はデコードコストのパレートフロンティアを目指しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/HuggingPapers/status/1952038716488208409">Twitter</a>。</p>
</li>
<li>
<p><strong>トレーニング/最適化</strong>  </p>
</li>
<li>GSPO（QwenによるRLアライメント）が注目されています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/SergioPaniego/status/1952305247411691871">Twitter</a>。  </li>
<li>MicrosoftはDion（Muon/MuPオプションとTritonカーネルを備えた分散オプティマイザ）をリリースしました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/jxbz/status/1951806916440854982">Twitter</a>、<a href="https://twitter.com/JingyuanLiu123/status/1951885788855345221">@JingyuanLiu123</a>。  </li>
<li>
<p>Hugging FaceのUltra-Scale Playbook（200+ページ、4,000+スケーリング実験）は5D並列処理、ZeRO、FlashAttn、オーバーラップ、ボトルネックをカバーしています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/Thom_Wolf/status/1951581743607070851">Twitter</a>、<a href="https://twitter.com/ClementDelangue/status/1952048356710039700">@ClementDelangue</a>。</p>
</li>
<li>
<p><strong>コーディング専門エコシステム</strong>  </p>
</li>
<li>Qwen3-CoderはCerebras上で「17倍速く」動作し、無料で試すことができます。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/SarahChieng/status/1951453803905163693">Twitter</a>。  </li>
<li>Fireworks上で利用可能な小型高速バリアント（Qwen3-Coder-Flash、GLM-4.5-Air）は、低遅延タスクにおいて競争力のあるツール使用品質を提供します。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/dzhulgakov/status/1952049826067050735">Twitter</a>。</li>
</ul>
<hr />
<p><strong>エージェントシステムとコーディング：Claude Codeの進化、インフラの成熟、"深いエージェント"パターン</strong></p>
<ul>
<li>
<p><strong>Claude Codeのアップデート</strong><br />
  Microcompact（古いツールコールを自動クリアしてセッションを延長）、@メンションとエージェントごとのモデル選択を備えたサブエージェント、ネイティブPDF取り込みが追加されました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/_catwu/status/1952488684579873195">Twitter</a>。<br />
  コンテキスト/剪定は一般的な調整の課題として残っており、複数のユーザーが冗長性と簡潔さのトレードオフを強調しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/giffmana/status/1952434564472644016">Twitter</a>。</p>
</li>
<li>
<p><strong>エコシステム</strong>  </p>
</li>
<li>Cline × Cerebrasのハッカソンは800+の開発者を集めました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/CerebrasSystems/status/1952511328964509794">Twitter</a>。  </li>
<li>TogetherのモデルスイートがOpencodeに追加されました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/togethercompute/status/1952495692557046141">Twitter</a>。  </li>
<li>KiloがGLM-4.5を統合しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/Zai_org/status/1952390223742255504">Twitter</a>。  </li>
<li>AmpとClaude Codeの詳細な比較が近日公開予定です。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/isaac_flath/status/1952399160579366957">Twitter</a>。  </li>
<li>
<p>Lindy 3.0がAgent Builder、Autopilot、チームコラボレーションを提供開始しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/Altimor/status/1952414217187086441">Twitter</a>。</p>
</li>
<li>
<p><strong>デザインパターン</strong>  </p>
</li>
<li>「深いエージェント」（LangChain）は仮想ファイルシステム状態を持つマルチステップサブエージェントを形式化します。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/hwchase17/status/1952408450878918834">Twitter</a>。  </li>
<li>Reflective prompt evolutionは複合システムでRLに匹敵する可能性があります。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/CShorten30/status/1952376642283708788">Twitter</a>、<a href="https://twitter.com/corbtt/status/1952437149544144984">@corbtt</a>。  </li>
<li>
<p>メモリはエージェントの個別化と効率性において重要なインフラとして浮上しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/_philschmid/status/1952370348600533000">Twitter</a>。</p>
</li>
<li>
<p><strong>ポリシー摩擦</strong><br />
  AnthropicはOpenAIのClaude CodeアクセスをToS違反と内部使用の多さを理由に制限しましたが、安全評価/ベンチマークのためのAPIアクセスは維持しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/sammcallister/status/1951642025381511608">Twitter</a>。</p>
</li>
</ul>
<hr />
<p><strong>マルチモーダル生成とビデオ：Grok Imagine、Runway Aleph、Veo 3、リアルタイム化に向けて</strong></p>
<ul>
<li>
<p><strong>Grok Imagineの展開</strong><br />
  xAIの画像/ビデオ生成がアプリ内で利用可能になりました（最初はウェイトリスト、その後Premium+とPremium）。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/tetsuoai/status/1951444393065586840">Twitter</a>、<a href="https://twitter.com/tobi/status/1951789462268391749">@tobi</a>、<a href="https://twitter.com/chaitualuru/status/1952174534142067092">@chaitualuru</a>、<a href="https://twitter.com/obeydulX/status/1951724900198367515">@obeydulX</a>。<br />
  Elon Muskは6秒のクリップを15秒でレンダリングすることを報告し、3〜6ヶ月以内にリアルタイム化を目指しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/elonmusk/status/1951883927582552547">Twitter</a>、<a href="https://twitter.com/elonmusk/status/1951516837906202782">@elonmusk</a>。</p>
</li>
<li>
<p><strong>Runway Aleph</strong><br />
  一般リリース（ウェブ+API）で、教室での採用が進んでいます（USC/UPenn）。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/runwayml/status/1951634909501575659">Twitter</a>、<a href="https://twitter.com/c_valenzuelab/status/1951663311503688018">@c_valenzuelab</a>、<a href="https://twitter.com/c_valenzuelab/status/1951568696155017286">@c_valenzuelab</a>。<br />
  コミュニティ実験ではマルチステップコンポジットと「無限UI」制御パラダイムが示されています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/c_valenzuelab/status/1952419024291188794">Twitter</a>。</p>
</li>
<li>
<p><strong>Veo 3画像からビデオへ</strong><br />
  Video Arena Discordでサイドバイサイドテストが可能です。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/lmarena_ai/status/1952052092719517729">Twitter</a>。Video Arenaは広範なモデル比較と投票を招待しています。</p>
</li>
</ul>
<hr />
<p><strong>オープンツール、インフラ、そして「オープンモデルを国家の優先事項に」推進</strong></p>
<ul>
<li>
<p><strong>オープンインフラの成熟</strong><br />
  Hugging Face Inferenceは「オープンウェイトインフラ」をプロプライエタリAPIに匹敵するレベルに押し上げています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/ClementDelangue/status/1951668724848599143">Twitter</a>。JanはHFをリモートプロバイダーとして追加しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/jandotai/status/1952248389531570333">Twitter</a>。Qdrant Edgeは埋め込みベクトル検索のプライベートベータに入りました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/qdrant_engine/status/1951631317939990765">Twitter</a>。Modalは一般目的の計算プラットフォームであり、推論専用ではないと再確認しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/bernhardsson/status/1951729049866514508">Twitter</a>。</p>
</li>
<li>
<p><strong>ATOMプロジェクト（米国オープンモデル）</strong><br />
  中国からの夏の急増後、オープンモデルリーダーシップを取り戻すための米国投資を求める声が研究者間で支持を集めています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/natolambert/status/1952370970762871102">Twitter</a>、<a href="https://twitter.com/Miles_Brundage/status/1952400404668657966">@Miles_Brundage</a>、<a href="https://twitter.com/finbarrtimbers/status/1952401883391520794">@finbarrtimbers</a>。VentureBeatのオピニオン記事は「オープンが重要」と主張しています。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/bgurley/status/1952031129143591234">Twitter</a>。</p>
</li>
<li>
<p><strong>データ/運用</strong><br />
  GoogleのAlphaEvolveはLLM駆動のテストループコード進化が新しいカーネルとインフラの成果を生み出し、トレーニング時間を1%削減しました。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/DeepLearningAI/status/1952112235196678274">Twitter</a>。RAG衛生の進展には、内部と外部ソースの階層的再ランキングが含まれ、幻覚を減少させます。詳細は以下のリンクをご覧ください：<a href="https://twitter.com/qdrant_engine/status/1951978606617326011">Twitter</a>。</p>
</li>
</ul>
<hr />
<p><strong>トップツイート（エンゲージメント順）</strong></p>
<ul>
<li><a href="https://twitter.com/nearcyan/status/1951926555934073147">@nearcyan</a>: 「次に何が起こるか信じられない」  </li>
<li><a href="https://twitter.com/sama/status/1951695003157426645">@sama</a>: 「次の数ヶ月で多くのものをローンチする予定」  </li>
<li><a href="https://twitter.com/elonmusk/status/1951516837906202782">@elonmusk</a>と<a href="https://twitter.com/elonmusk/status/1951883927582552547">レンダーアップデート</a>: Grok Imagineとほぼリアルタイムビデオについて  </li>
<li><a href="https://twitter.com/karpathy/status/1951577221753094399">@karpathy</a>: 「2024年：みんなChatをリリース。2025年：みんなCodeをリリース」  </li>
<li><a href="https://twitter.com/gdb/status/1951882297172779336">@gdb</a>: 「ソフトウェアエンジニアであることがこれまで以上に楽しい」  </li>
<li><a href="https://twitter.com/LHSummers/status/1951998034973163940">@LHSummers</a>: 統計の政治化について  </li>
<li><a href="https://twitter.com/balajis/status/1951515516939673996">@balajis</a>: 「古いTwitterのようにAIにプロンプトを与える」  </li>
<li><a href="https://twitter.com/demishassabis/status/1951468051578142848">@demishassabis</a>: Gemini 2.5 Deep Thinkの発表  </li>
<li><a href="https://twitter.com/Yuchenj_UW/status/1951677714173477031">@Yuchenj_UW</a>: Thinkyの1.5Bドルの拒否とMVP保持について  </li>
<li><a href="https://twitter.com/OpenAI/status/1952414411131671025">@OpenAI</a>: 「後悔のない時間」のためのChatGPTの最適化  </li>
<li><a href="https://twitter.com/naval/status/1951900029389820253">@naval</a>: 「良いチームは保持する製品よりもはるかに多くの製品を捨てる」  </li>
<li><a href="https://twitter.com/paulg/status/1952155863864733750">@paulg</a>: リンクの優先順位低下がウェブの最高のコンテンツを隠す  </li>
</ul>
<hr />
<h1 id="ai-reddit-recap">AI Reddit Recap</h1>
<h2 id="rlocalllama-rlocalllm-recap">/r/LocalLlama + /r/localLLM Recap</h2>
<h3 id="1-qwen-image-20b">1. Qwen-Image 20Bモデルリリースとベンチマーク</h3>
<ul>
<li>
<p><a href="https://huggingface.co/Qwen/Qwen-Image"><strong>QWEN-IMAGEがリリースされました！</strong></a><br />
<strong>QWEN-IMAGEは新しくリリースされたビジョンモデルで、内部ベンチマークでFlux Kontext Proを上回る性能を示しています。</strong><br />
  詳細な技術的議論では、QWEN-IMAGEがオブジェクト検出、セマンティックセグメンテーション、深度/エッジ（Canny）推定、新しいビュー合成、超解像などの幅広い画像理解タスクをサポートしていることが強調されています。  </p>
</li>
<li>
<p><a href="https://i.redd.it/7a463it8z0hf1.jpeg"><strong>🚀 Meet Qwen-Image</strong></a><br />
<strong>Qwen-Imageは、テキストと画像の組み合わせにおいて高い忠実度を示し、以前のオープンモデルを上回る性能を発揮しています。</strong>  </p>
</li>
<li>
<p><a href="https://v.redd.it/4077mfg081hf1"><strong>Qwen-Imageが公開されました</strong></a><br />
<strong>AlibabaはQwen-Imageをリリースしました。この新しいマルチモーダルモデルは、Twitterで発表されました。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"><strong>Qwen Image 20Bが登場します！</strong></a><br />
<strong>AlibabaのQwenチームがQwen Image 20Bモデルをリリース予定です。</strong>  </p>
</li>
<li>
<p><a href="https://i.redd.it/qemmgysvuzgf1.png"><strong>今日の新しいQwenモデル！</strong></a><br />
<strong>新しいQwenモデルが間もなくリリースされることが発表されました。</strong>  </p>
</li>
</ul>
<h3 id="2-llmpangu-ultrahunyuan">2. 主要な中国のLLMリリース：Pangu UltraとHunyuan</h3>
<ul>
<li>
<p><a href="https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md"><strong>HuaweiがPangu Ultraのウェイトを公開しました</strong></a><br />
<strong>Huaweiは718BパラメータのMixture-of-Experts（MoE）モデルであるPangu Ultraのウェイトを公開しました。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"><strong>新しいHunyuan Instructモデル</strong></a><br />
<strong>TencentがHunyuan-Instructシリーズのオープンソース言語モデルをリリースしました。</strong>  </p>
</li>
</ul>
<h3 id="3-qwen">3. メタディスカッションとミーム：Qwenモデルの公開とコミュニティの反応</h3>
<ul>
<li>
<p><a href="https://i.redd.it/g7t8cmgrv0hf1.jpeg"><strong>Sam AltmanがQwenのモデル公開を見守る様子</strong></a><br />
<strong>Sam AltmanがQwenの急速なモデル公開を見守る様子を描いたミームです。</strong>  </p>
</li>
<li>
<p><a href="https://i.redd.it/f0xr7mshc0hf1.png"><strong>現在のr/LocalLLaMAの状況</strong></a><br />
<strong>オープンソースLLMの競争状況を描いたミームです。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/"><strong>Horizon BetaはOpenAI（さらなる証拠）</strong></a><br />
<strong>Horizon BetaモデルがOpenAI技術に基づいている可能性があることを示す証拠が提示されました。</strong>  </p>
</li>
</ul>
<h2 id="less-technical-ai-subreddit-recap">Less Technical AI Subreddit Recap</h2>
<h3 id="1-qwen-image">1. Qwen-Imageモデルリリースとベンチマーク</h3>
<ul>
<li>
<p><a href="https://huggingface.co/Qwen/Qwen-Image"><strong>Qwen-Imageがリリースされました</strong></a><br />
<strong>AlibabaがQwen-Imageをリリースしました。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/StableDiffusion/comments/1mhe9jb/qwen_image_is_coming/"><strong>Qwen Imageが登場します！</strong></a><br />
<strong>AlibabaのQwenチームがQwen Image 20Bモデルをリリース予定です。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/gallery/1mhikh2"><strong>Qwen ImageはFlux Kontext Proよりも画像編集で優れています</strong></a><br />
<strong>Qwen-Imageモデルが画像生成と編集タスクでSOTA性能を示しています。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/StableDiffusion/comments/1mhkmsa/warning_pickle_virus_detected_in_recent_qwenimage/"><strong>警告：最近のQwen-Image NF4にピクルウイルスが検出されました</strong></a><br />
<strong>HuggingFaceモデルリポジトリが潜在的な「ピクルウイルス」を含むとして警告されました。</strong>  </p>
</li>
</ul>
<h3 id="2-claude-41-opus">2. Claude 4.1 &amp; Opus次世代モデルのローンチ期待</h3>
<ul>
<li>
<p><a href="https://i.redd.it/a2kwuoauf0hf1.png"><strong>Claude 4.1 Opusも間もなく登場するようです</strong></a><br />
<strong>Claude 4.1 Opusのリリースが近いことを示唆する投稿です。</strong>  </p>
</li>
<li>
<p><a href="https://i.redd.it/a2kwuoauf0hf1.png"><strong>Opus 4.1が登場するのか？</strong></a><br />
<strong>Opus 4.1モデルのリリースが近い可能性があります。</strong>  </p>
</li>
<li>
<p><a href="https://v.redd.it/i9aehe15axgf1"><strong>これはClaude Opus 4で見た中で最も驚くべきワンショットの一つです</strong></a><br />
<strong>Claude Opus 4が単一の推論で複雑なHTMLドローンシミュレーターを生成しました。</strong>  </p>
</li>
</ul>
<h3 id="3-gpt-5openai">3. GPT-5リリースの兆候とOpenAIの発表</h3>
<ul>
<li>
<p><a href="https://i.redd.it/2d0vdv8spzgf1.png"><strong>木曜日がGPT-5の日になるようです（信頼できるJimmyによると）</strong></a><br />
<strong>GPT-5のリリース日が木曜日になる可能性があります。</strong>  </p>
</li>
<li>
<p><a href="https://i.redd.it/bja36ao2w0hf1.jpeg"><strong>OpenAI VP of ChatGPT：「大きな週が始まる」</strong></a><br />
<strong>ChatGPTのOpenAI VPが「大きな週が始まる」と発言しました。</strong>  </p>
</li>
<li>
<p><a href="https://www.reddit.com/r/singularity/comments/1mhkahr/gpt5_easter_egg/"><strong>GPT-5イースターエッグ</strong></a><br />
<strong>OpenAIの従業員がGPT-5に関するヒントをツイートしました。</strong>  </p>
</li>
</ul>
<hr />
    </main>

    <footer>
        <p>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
            <a href="https://news.smol.ai/">news.smol.ai</a>
        </p>
    </footer>
</body>
</html>