<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>静かな一日 | AIニュース</title>
    <meta name="description" content="静かな一日 - AIニュース 2025-07-24。最新のAI技術動向を日本語でお届け。">
    <meta name="keywords" content="AI,人工知能,ニュースレター,2025-07-24,機械学習,深層学習,日本語">
    <meta name="author" content="AIニュース">
    <link rel="canonical" href="https://yipg.github.io/ainews/docs/newsletters/2025-07-24.html">
    
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="静かな一日 | AIニュース">
    <meta property="og:description" content="静かな一日 - AIニュース 2025-07-24。最新のAI技術動向を日本語でお届け。">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yipg.github.io/ainews/docs/newsletters/2025-07-24.html">
    <meta property="og:site_name" content="AIニュース">
    <meta property="og:locale" content="ja_JP">
    <meta property="article:published_time" content="2025-07-24T09:00:00+00:00">
    <meta property="article:author" content="AIニュース">
    <meta property="article:section" content="AI技術ニュース">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="静かな一日 | AIニュース">
    <meta name="twitter:description" content="静かな一日 - AIニュース 2025-07-24。最新のAI技術動向を日本語でお届け。">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>✏️</text></svg>">
    <link rel="alternate icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA">
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="AIニュース RSS Feed" href="../feed.xml">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Verdana, Geneva, sans-serif;
            font-size: 1em;
            line-height: 1.7;
            letter-spacing: 0.02em;
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            color: #111;
            word-wrap: break-word;
        }
        
        nav {
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: nowrap;
        }
        
        .site-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #111;
            text-decoration: none;
            flex-shrink: 0;
        }
        
        .site-title:hover {
            text-decoration: underline;
        }
        
        .nav-links {
            font-size: 0.85em;
            white-space: nowrap;
        }
        
        .nav-links a {
            color: #111;
            text-decoration: none;
            margin-left: 12px;
        }
        
        .nav-links a:hover {
            text-decoration: underline;
        }
        
        
        h1, h2, h3, h4, h5, h6 {
            margin: 35px 0 20px 0;
            line-height: 1.3;
            color: #111;
            letter-spacing: 0.01em;
        }
        
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.3em; }
        h3 { font-size: 1.1em; }
        
        p {
            margin: 20px 0;
        }
        
        a {
            color: #0969da;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        blockquote {
            border-left: 3px solid #ccc;
            margin: 25px 0;
            padding: 0 25px;
            color: #555;
            font-style: italic;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 25px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 25px 0;
            border-radius: 3px;
        }
        
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 35px 0;
        }
        
        footer {
            margin-top: 40px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #555;
            font-size: 0.85em;
        }
        
        footer a {
            color: #555;
            text-decoration: none;
            margin: 0 8px;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 15px;
                font-size: 0.95em;
            }
            
            nav {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            
            .nav-links {
                font-size: 0.8em;
            }
            
            .nav-links a {
                margin-left: 0;
                margin-right: 12px;
            }
            
            .article-title {
                font-size: 1.4em;
            }
        }
        
        @media (max-width: 480px) {
            body {
                font-size: 0.9em;
                padding: 12px;
            }
            
            .site-title {
                font-size: 1em;
            }
            
            .nav-links {
                font-size: 0.75em;
            }
            
            .article-title {
                font-size: 1.3em;
            }
        }
        
        @media (prefers-color-scheme: dark) {
            body {
                background-color: #111;
                color: #eee;
            }
            
            nav {
                border-bottom-color: #444;
            }
            
            .site-title, .nav-links a, .article-title, h1, h2, h3, h4, h5, h6 {
                color: #eee;
            }
            
            .article-date {
                color: #ccc;
            }
            
            blockquote {
                border-left-color: #555;
                color: #ccc;
            }
            
            code {
                background-color: #2d3748;
                color: #e2e8f0;
            }
            
            pre {
                background-color: #2d3748;
            }
            
            hr, footer {
                border-color: #444;
            }
            
            footer, footer a {
                color: #ccc;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html" class="site-title">✏️ AIニュース</a>
        <div class="nav-links">
            <a href="../index.html">ホーム</a>
            <a href="./archive.html">アーカイブ</a>
            <a href="../feed.xml">RSS</a>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
        </div>
    </nav>

    <main>
        <h1 id="_1">静かな一日</h1>
<blockquote>
<p>AIニュース（2025年7月22日～2025年7月23日）。ホワイトハウスは<a href="https://www.ai.gov/action-plan">AIアクションプラン</a>を発表しましたが、このニュースレターでは技術的な内容に焦点を当てます。昨日コメントしたように、QwenCoderは概ね好意的な評価を受けていますが、タイトルストーリーにするほどの注目度ではありません。</p>
</blockquote>
<hr />
<h1 id="ai-twitter-recap">AI Twitter Recap</h1>
<p><strong>新モデルリリース: Qwen3-Coder</strong></p>
<ul>
<li><strong>ローンチと性能主張</strong>: <a href="https://twitter.com/bigeagle_xd/status/1947817705324621910">@Alibaba_Qwen</a>が<strong>Qwen3-Coder-480B-A35B-Instruct</strong>を発表しました。このモデルは<strong>480B</strong>の総パラメータ（<strong>35B</strong>がアクティブ）と<strong>256K</strong>のコンテキスト長を持つオープンソースのコードモデルです。初期レポートではSOTA（最先端技術）性能を主張しており、<a href="https://twitter.com/ClementDelangue/status/1947775783067603188">@itsPaulAi</a>は「これまで見た中で最高のコーディングモデルの一つ」と評価しました。このモデルはわずか3か月で開発されたことが<a href="https://twitter.com/scaling01/status/1947773545733394439">@scaling01</a>によって強調されました。<a href="https://twitter.com/AravSrinivas/status/1947810865685925906">@AravSrinivas</a>は「驚異的な結果！オープンソースが勝利している」と祝福しました。</li>
<li><strong>ベンチマーク論争</strong>: ベンチマークスコアに関する論争が発生しました。公式リリースでは<strong>ARC-AGI-1で41.8%</strong>を主張しましたが、<a href="https://twitter.com/fchollet/status/1947821353358483547">@fchollet</a>は彼のチームが公開または半公開の評価セットでこのスコアを再現できなかったと述べ、他の最近のベースモデルと同程度の性能であると指摘しました。彼はARC Prize Foundationによって検証されたスコアのみを信頼するよう促しました。<a href="https://twitter.com/clefourrier/status/1947994251410682198">@GregKamradt</a>も結果の再現性について公開質問を行いました。</li>
<li><strong>エコシステム統合</strong>: モデルは迅速にエコシステム全体に統合されました。<a href="https://twitter.com/vllm_project/status/1947780382847603053">@vllm_project</a>は<strong>vLLM nightly</strong>での専門的な並列処理のサポートを発表しました。<a href="https://twitter.com/QuixiAI/status/1947773516368994320">@UnslothAI</a>は<strong>Dynamic GGUFs</strong>をアップロードし、最大<strong>1Mコンテキスト長</strong>を提供しました。また、<a href="https://twitter.com/huybery/status/1947808085504102487">@OpenRouterAI</a>、<a href="https://twitter.com/Alibaba_Qwen/status/1947954292738105359">@cline</a>、<a href="https://twitter.com/vipulved/status/1947871449282216055">@togethercompute</a>で利用可能となりました。<a href="https://twitter.com/ClementDelangue/status/1947780025886855171">@ClementDelangue</a>はモデルを試すためのウェブ開発スペースを強調しました。</li>
<li><strong>技術分析</strong>: <a href="https://twitter.com/rasbt/status/1947995162782638157">@rasbt</a>は、このリリースがコーディングにおいて「<strong>専門化が一般目的モデルに勝る</strong>」ことを示しているとコメントしました。<a href="https://twitter.com/cline/status/1948072664075223319">@cline</a>は<strong>Qwen3-Coder</strong>が<strong>Kimi K2</strong>をわずか2週間で上回り、サイズが半分でコンテキストが2倍であることを観察し、オープンソースモデルが「脱出速度」に達していると示唆しました。</li>
</ul>
<p><strong>米国AI政策と地政学</strong></p>
<ul>
<li><strong>アメリカのAIアクションプラン</strong>: <strong>ホワイトハウス</strong>は「AI競争に勝つこと」に焦点を当てた新しい<strong>AIアクションプラン</strong>を発表しました。<a href="https://twitter.com/scaling01/status/1948037110662848925">@scaling01</a>はその詳細な要約を提供し、<strong>イノベーション</strong>、<strong>インフラ</strong>、<strong>国際外交</strong>の3つの柱を概説しました。主要指令には<strong>NIST AIリスク管理フレームワーク</strong>の改訂、客観的モデルの開発者との政府契約の確保、そして「アメリカの価値観に基づいたオープンモデル」の促進が含まれます。</li>
<li><strong>国家安全保障とインフラ</strong>: この計画はAIの優位性を国家安全保障と明確に結びつけており、<a href="https://twitter.com/scaling01/status/1948038740405879206">@scaling01</a>は国家緊急時に<strong>国防総省（DOD）</strong>に計算資源への優先アクセスを付与することを指摘しました。また、「1970年代以降アメリカのエネルギー能力は停滞している一方で、中国は急速に電力網を構築している」と述べ、この傾向をAIの優位性のために変える必要があるとしています。この計画はまた、中国の影響力に対抗し、敏感な技術に対する輸出規制を課す措置を詳述しています。</li>
<li><strong>オープン対クローズドソースの議論</strong>: 計画の発表はオープンソースAIに関する議論を激化させました。<a href="https://twitter.com/ClementDelangue/status/1948037061304356901">@ClementDelangue</a>は「<strong>『オープンは安全ではない』という馬鹿げた話をやめる時だ</strong>」と主張し、AI競争に負けないためにオープンサイエンスに戻るべきだと述べました。これに対して<a href="https://twitter.com/Yuchenj_UW/status/1947866064500756579">@Yuchenj_UW</a>は「<strong>米国はクローズドソースAIのみを出荷している</strong>」一方で「<strong>中国はオープンソースAIのみを出荷している</strong>」と観察しました。<a href="https://twitter.com/Teknium1/status/1947820839178817741">@Teknium1</a>は、この計画が「オープンウェイト」AIモデルの開発を奨励していることを強調しました。</li>
</ul>
<p><strong>モデルアップデート、研究、技術</strong></p>
<ul>
<li><strong>LLMにおける潜在学習</strong>: <a href="https://twitter.com/EthanJPerez/status/1947839794513604768">@OwainEvans_UK</a>と<strong>Anthropic Fellows</strong>による論文が「潜在学習」の概念を紹介しました。この研究では、LLMがデータを通じて隠れた特性を他のモデルに伝達できることを示しています。<a href="https://twitter.com/swyx/status/1947875989666832576">@swyx</a>はこれが価値体系を輸出する強力な「<strong>ソフトパワーツール</strong>」になる可能性があると示唆し、<a href="https://twitter.com/giffmana/status/1948092020834083001">@giffmana</a>はこれを一般化と蒸留に関する研究と解釈しました。</li>
<li><strong>Geminiアップデート</strong>: <a href="https://twitter.com/zacharynado/status/1947805002585792682">@OfficialLoganK</a>は<strong>Gemini 2.5 Flash-Lite</strong>が安定し、プロダクション利用の準備が整ったことを発表しました。<a href="https://twitter.com/zacharynado/status/1947886752154425">@sundarpichai</a>はその性能を<strong>400トークン/秒</strong>とコスト効率を強調しました。<a href="https://twitter.com/dl_weekly/status/1948105084480397503">@GoogleDeepMind</a>は<strong>Gemini with Deep Think</strong>が<strong>国際数学オリンピック（IMO）</strong>で金メダル基準を達成したことを明らかにしました。</li>
<li><strong>新しい音声およびテキスト読み上げ（TTS）モデル</strong>: <a href="https://twitter.com/ClementDelangue/status/1948021500587491538">@reach_vb</a>は<strong>Higgs Audio V2</strong>のリリースを共有しました。このモデルは<strong>@boson_ai</strong>によるオープンで統一されたTTSモデルで、GPT-4o mini TTSやElevenLabs v2を上回るとされています。<a href="https://twitter.com/reach_vb/status/1948012058630303857">@reach_vb</a>は単一モデルからの音声クローンで複数人生成が可能であることを示しました。<strong>Mistral AI</strong>も<a href="https://twitter.com/andrew_n_carr/status/1947779499032285386">Voxtral Technical Report</a>をリリースしました。</li>
<li><strong>その他の注目すべきリリースと研究</strong>: <strong>Moonshot AI</strong>の<strong>Kimi K2</strong>が<strong>Chatbot Arena</strong>で#1を獲得し、同社は現在<a href="https://twitter.com/Kimi_Moonshot/status/1947977043469340801">複数の役職を積極的に採用中</a>です。<strong>Neta AI</strong>は<a href="https://twitter.com/ClementDelangue/status/1947783259028430864"><strong>Neta Lumina</strong></a>というオープンソースのアニメモデルを立ち上げました。<a href="https://twitter.com/Tim_Dettmers/status/1947783030837240265"><strong>StellaLisy</strong></a>による研究はブラックボックスの嗜好モデルを超えた人間の意思決定の分解を探求しました。</li>
<li><strong>RLとコンテキストエンジニアリング</strong>: <a href="https://twitter.com/shaneguML/status/1947858876239646909">@shaneguML</a>は、バックプロパゲーションが彼を失敗させた後、2016年にRLを追求した理由を共有しました。<a href="https://twitter.com/omarsar0/status/1947859083702239314">@omarsar0</a>は、現在のコーディングモデルで不足しているのは生のモデル能力ではなく、<strong>賢いメモリ管理とコンテキストエンジニアリング</strong>であることを強調しました。</li>
</ul>
<p><strong>AIツール、フレームワーク、インフラ</strong></p>
<ul>
<li><strong>Perplexity Cometブラウザ</strong>: <a href="https://twitter.com/AravSrinivas/status/1947892351831056886">@AravSrinivas</a>は、2030年にChromeをまだ使用しているかどうかを尋ね、Perplexityの<strong>Comet</strong>ブラウザの文脈で議論を巻き起こしました。彼はCometの<a href="https://twitter.com/AravSrinivas/status/1947817943934587362">Chromeより優れたメモリ管理</a>とユーザーが<a href="https://twitter.com/AravSrinivas/status/1948056269958648309">エージェントのようにすべてを検索できる</a>能力を強調しました。また、<a href="https://twitter.com/AravSrinivas/status/1948102473597829200">広告ブロッカーが拡張機能なしでネイティブに動作する</a>ことを明確にしました。</li>
<li><strong>Claude Codeが「万能エージェント」に</strong>: <strong>Claude Code</strong>が多用途で強力なツールになりつつあるという強い意見が浮上しました。<a href="https://twitter.com/alexalbert__/status/1948060675974283689">@alexalbert__/</a>はこれを「<strong>万能エージェント</strong>」と宣言しました。その<strong>PostHog</strong>内での統合も<a href="https://twitter.com/swyx/status/1947829167707590663">@swyx</a>によって注目されました。</li>
<li><strong>主要インフラ契約</strong>: 巨大なインフラプレイとして、<a href="https://twitter.com/mckbrando/status/1947874429972926905">@sama</a>は<strong>OpenAI</strong>が<strong>Stargate</strong>プロジェクトの一環として<strong>Oracle</strong>と追加の<strong>4.5ギガワット</strong>の容量契約を締結したことを確認しました。</li>
<li><strong>フレームワークとライブラリのアップデート</strong>:</li>
<li><strong>vLLM</strong>: プロジェクトは<a href="https://twitter.com/ClementDelangue/status/1947775555387916397">Vision-Language Modelsがサポートされる</a>ことを発表しました。</li>
<li><strong>OpenCLIP &amp; timm</strong>: <a href="https://twitter.com/wightmanr/status/1948108826206707744">@wightmanr</a>は共同リリースを発表し、ヘッドライン機能としてOpenCLIPでの<strong>Perception Encoder (PE) Core</strong>サポートとtimmでの<strong>NaFlexViT ROPE</strong>サポートを強調しました。</li>
<li><strong>Gradio</strong>: <a href="https://twitter.com/_akhaliq/status/1947988902079279126"><strong>Gradio</strong>がGoogle Colabにプリインストールされた</a>ことが発表され、ノートブックでのデモ作成プロセスが簡素化されました。</li>
<li><strong>LangChain</strong>: <a href="https://twitter.com/hwchase17/status/1947786031778173022">@hwchase17</a>は<strong>Bedrock AgentCore</strong>ツールの<strong>LangGraph</strong>エージェントとの新しい統合を強調しました。</li>
<li><strong>LlamaCloud</strong>: <a href="https://twitter.com/jerryjliu0/status/1947819412146291161">@jerryjliu0</a>はAIエージェントのクリーンなドキュメントコンテキストを確保するための新しい<strong>ヘッダー/フッター検出</strong>機能を導入しました。</li>
</ul>
<p><strong>企業、エコシステム、広範な影響</strong></p>
<ul>
<li><strong>人間とコンピュータの相互作用の未来</strong>: <a href="https://twitter.com/karpathy/status/1948062129187140051">@karpathy</a>は<strong>Tesla Supercharger diner</strong>の写真を共有し、それを「未来の展示」と呼びました。<a href="https://twitter.com/OriolVinyalsML/status/1948075766417367417">@OriolVinyalsML</a>は挑発的に「<strong>私たちはすでに</strong>エイリアン知性と話している。ただし、かなり狭いコミュニケーションボトルネックを通じて」と述べました。同様に、<a href="https://twitter.com/GoogleDeepMind/status/1948098855053979930">@DemisHassabis</a>は、AIがタンパク質の折りたたみのような自然なパターンを学ぶことができれば、科学的発見の新しい時代を解き放つ可能性があると議論しました。</li>
<li><strong>企業のマイルストーンと資金調達</strong>: AI回路基板設計会社<strong>Diode</strong>は<a href="https://twitter.com/espricewright/status/1948064649867632691">a16z主導の<strong>1140万ドルのシリーズA</strong>を調達</a>したことを発表しました。ビデオ生成会社<strong>Synthesia</strong>は<a href="https://twitter.com/synthesiaIO/status/1948007255330132133">初の<strong>100万ドル以上の日</strong>を達成</a>したことを発表しました。</li>
<li><strong>科学のためのAI</strong>: <strong>Google</strong>は古代ラテン語碑文を<a href="https://twitter.com/Google/status/1948039522194718799">文脈化するための新しいAIモデル<strong>Aeneas</strong></a>を発表しました。<strong>AI at Meta</strong>は、<a href="https://twitter.com/AIatMeta/status/1948042281107538352">神経信号をコンピュータコマンドに変換するための高度なMLモデルとEMGハードウェアを使用した研究</a>を発表しました。</li>
<li><strong>AIのコスト</strong>: <a href="https://twitter.com/vikhyatk/status/1947875363889287179">@vikhyatk</a>は<strong>Sonnet</strong>の使用コストを比較しました。PyTorchモジュールの作成には<strong>0.038ドル</strong>、Reactコンポーネントの作成には<strong>33.74ドル</strong>かかるとしています。</li>
</ul>
<p><strong>ユーモア/ミーム</strong></p>
<ul>
<li><strong>文化的コメント</strong>: <a href="https://twitter.com/Teknium1/status/1947811854665060552">@Teknium1</a>は、大阪でイベント出口を示すために使用されるドローンのビデオを共有しました。1981年のShel Silversteinの予言的な漫画が<a href="https://twitter.com/nptacek/status/1947858160259146085">@nptacek</a>によって共有されました。</li>
<li><strong>業界の風刺</strong>: <a href="https://twitter.com/scaling01/status/1947997712542322733">@scaling01</a>は「<strong>あなたは中国のAI研究者を匿っていますね？</strong>」というキャプション付きのミームを投稿しました。<a href="https://twitter.com/tamaybes/status/1947866741541113957">@tamaybes</a>は「<strong>AIモデルにフランス語の名前を付けると、年間20%オフラインになるのは驚くべきことではないかもしれません。</strong>」とジョークを言いました。</li>
<li><strong>コミュニティのジョーク</strong>: <a href="https://twitter.com/scaling01/status/1948053713865916817">@scaling01</a>は著名な研究者からの「いいね」を祝福し、「<strong>なんてことだ、Sholtoが私の投稿を気に入った</strong>」とコメントしました。<a href="https://twitter.com/AravSrinivas/status/1947817943934587362">@AravSrinivas</a>はPerplexity Cometが「<strong>Chromeより優れたメモリ管理</strong>」を持っていると冗談を言いました。</li>
<li><strong>共感できるコンテンツ</strong>: <a href="https://twitter.com/Yuchenj_UW/status/1948079112427548792">@Yuchenj_UW</a>は古いソフトウェアUIの懐かしい画像を「<strong>これが私たちから奪われたものです</strong> 😢」というキャプションで投稿しました。</li>
</ul>
<hr />
<h1 id="ai-reddit-recap">AI Reddit Recap</h1>
<h2 id="rlocalllama-rlocalllm-recap">/r/LocalLlama + /r/localLLM Recap</h2>
<h3 id="1-qwen3qwen3-coder">1. Qwen3とQwen3-Coderのリリース性能、ベンチマーク、ユーザー体験</h3>
<ul>
<li><a href="https://i.redd.it/s9cwrvwg1jef1.png"><strong>Qwen3-Coder Unsloth dynamic GGUFs</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/">スコア: 259, コメント: 72</a>): <strong>この画像はQwen3-Coderのプロモーショングラフで、特に新しい480Bパラメータバリアントが動的GGUF量子化（2-8ビット、最大1Mコンテキスト長をサポートする182GB 2ビットモデルを含む）を使用していることを他の大規模言語モデルと比較しています。投稿はこれらの巨大モデルを効率的に実行するための戦略（llama.cpp MoEオフロード、フラッシュアテンション、KVキャッシュ量子化）を強調し、関連リソースと<a href="https://docs.unsloth.ai/basics/qwen3-coder">完全なドキュメントはこちら</a>、GGUFチェックポイントは<a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF">Huggingface</a>で利用可能です。</strong> トップコメントでは、これらの大規模モデルを効率的に処理するための高度なオフロード技術の必要性が指摘され、ハードウェア要件の課題とソフトウェア最適化の継続的な必要性が強調されています。</li>
<li>極めて大きなモデルサイズについての議論があり、特にQwen3-Coder Unsloth dynamic GGUFsの<code>180 GB</code> Q2_X_L量子化に関する技術的な比較が求められています。この議論は量子化レベル、ファイルサイズ、推論性能/リソースニーズのトレードオフを強調しています。</li>
<li>ユーザーはモデルサイズの大きさにより推論を消費者向けハードウェアに押し込むために「クレイジーなオフロードハック」が必要であると述べ、合理的な推論速度と能力を達成するために高度なメモリ管理、ストレージストリーミング、またはマルチGPU/CPU技術が必要であることを示唆しています。</li>
<li><a href="https://i.redd.it/8gjn0yhf1jef1.png"><strong>最近のQwenベンチマークスコアは疑問視されている</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/">スコア: 375, コメント: 66</a>): <strong>添付画像はARCベンチマークの作成者であるFrançois Cholletによるツイートを示しており、Qwen 3の41.8% ARC-AGI-1スコアの主張に疑問を呈しています。彼はこれらの結果を公開または半公開の評価セットで再現できなかったと述べ、最近の他のモデルとより一致していると示唆しました。CholletはARC Prize Foundationによって検証されたスコアのみを信頼するよう促し、評価方法論の一貫性と公平性に関する懸念を強調しました。Qwenチームのメンバーは異なる解析形式（JSON）の使用を明確にし、プライベートな再現を提供することで応答しました。</strong> コメントは、現代のベンチマークスコアに対する一般的な懐疑論を強調し、いくつかのモデル（例: EXAONE 4）が最近疑わしいほど高い結果を投稿していることを指摘しています。多くのユーザーは現在、実際の評価により重きを置いています。</li>
<li>最近のQwen3-235B-A22Bベンチマーク結果に対する懐疑論が浮上しており、あるユーザーは「驚くべきベンチマーク」が発表されているにもかかわらず、以前の235Bリリースに比べて非常に少ない改善しか観察されていないと指摘しています。これは、公開されたスコアと実際の性能との間の整合性に関する懸念を強調しています。</li>
<li>別のコメントでは、最近のベンチマークの信頼性に関する懸念が浮上しており、特にEXAONE 4 32Bが最近R1-0528をいくつかの指標で匹敵または上回ると主張していることを指摘しています。これは、LLM分野全体で疑わしいまたは誇張されたベンチマーク報告の傾向を示しています。実際の評価に基づく選択が好まれる傾向があります。</li>
<li>参照されたTwitterスレッドでは、Qwenチームがベンチマーク方法論に関する非難に応答し、JSONを使用した解析を明確にし、再現詳細を共有することを提案していることが示されています。これは透明性への試みを示唆し、公正な外部検証の困難さを強調しています。</li>
<li><a href="https://www.reddit.com/gallery/1m70n7q"><strong>AlibabaのアップグレードされたQwen3 235B-A22B 2507は現在最も知的な非推論モデルです。</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/">スコア: 258, コメント: 38</a>): <strong>AlibabaのQwen3 235B-A22B 2507モデルはArtificial Analysis Intelligence Indexで60のスコアを達成し、Claude 4 OpusやKimi K2（いずれも58）、DeepSeek V3 0324やGPT-4.1（いずれも53）を上回り、非推論前任者に比べて13ポイントの改善を示しています。特にQwen3 235B 2507は、以前のQwen3 235Bリリースに比べて3倍以上のトークンを使用し、「思考」モデルを超えるトークン使用を活用してこれを達成しています。</strong> コメントでは、このようなベンチマークの有効性と現実世界でのLLM選択への関連性について議論が行われています。Qwen3 235B-A22B 2507がこれらの指標で優れている一方で、Deepseekが現実世界の知識検索や創造的な文章作成で優れている可能性があると指摘されています。また、「思考」モデルと「非思考」モデルのカテゴリー間の区別に関する懐疑論もあります。</li>
<li>トークン使用についての活発な議論があります。Qwen3 235B-A22B 2507は以前の235B非思考モードに比べて大幅に多くのトークンを消費しており（「以前の235B非思考モードに比べて3倍以上」）、推論や出力品質のためのコンテキストやメモリ利用が大幅に増加していることを示唆しています。</li>
<li>実用的なベンチマークはタスクに依存します。ユーザーはQwen3 235B-A22B 2507が「非常に使いやすい」推論速度（ホームPCで約4トークン/秒）を提供し、特に複雑な仕事関連のクエリでChatGPTと競争力のある応答を提供することを指摘しています。ただし、一部のユーザーはDeepseekモデルがより優れた世界知識と創造的な文章作成能力を持っていると感じています。</li>
<li>性能対速度比は新しいQwen3モデルで称賛されていますが、技術ユーザーは依然としてKimi K2やDeepseek V3を低量子化（Q3）で好む傾向があり、Qwen3 235BをQ8で使用することに比べて量子化効率と現実世界のタスクでの評価の重要性を強調しています。</li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/"><strong>Qwen 3 Coderは私のテストではかなり良いです</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/">スコア: 180, コメント: 37</a>): <strong>ユーザーはQwen 3 Coder（OpenRouter経由、Alibabaクラウド推論、約60トークン/秒）を使用して、以前にKimi K2（Groq Q4）やClaude Sonnetで試みた半複雑なWeb ACL統合を高コンテキスト（1200行のアーキテクチャ、約30kプロンプトトークン）シナリオでテストしました。Qwen 3 Coderは信頼性が高く（「タスクを一発で完了」）、修正が必要なく、Kimi K2 Q4を上回り、このコンテキストではSonnet 4と同等と見なされました—オープンソースコードモデルの大きな進歩を示しています。主な欠点として、OpenRouter経由での推論コストが高い（1つの機能タスクで5ドル）ことが挙げられ、サブスクリプションLLM（例: Claude Pro/Sonnet 4月額）に比べてオープンモデルの使用可能性拡大に懸念が生じています。</strong> コメントでは、高いオープンモデル価格が競争の欠如、モデルサイズ/メモリ要件、プロバイダー側の補助金の欠如（AnthropicのClaudeスタックとは異なる）によるものであることが指摘されています。あるユーザーはACLセキュリティ原則の変更（デフォルト拒否アプローチ）を提案し、より堅牢なLLM駆動のコーディング結果を得ることを提案しています。</li>
<li>Qwen 3 Coder（OpenRouter経由）とClaude Codeの価格差は、Qwen 3の最近のローンチ（競争が始まる前にプロバイダーが高価格を設定できる）、その大きなモデルサイズが高いメモリ要求を引き起こすこと、そしてAnthropicが独自のスタックと資本を利用してClaude推論を補助できるという事実によるものとされています。補助金が消えると、価格差は縮小する可能性があります。（出典: md5nake）</li>
<li>技術的性能: ユーザーはMoonshot経由でAnthropicエンドポイントを使用すると高いキャッシュヒット率（約80%のトークンがキャッシュから提供される）を得られることを指摘し、実際のコストがリスト価格よりもはるかに低くなることを観察しました—「Claude Codeで25ドルと表示されていたのに実際には2ドルしかかかりませんでした」。彼らは強力なコーディング性能（約5k LOCを一度に処理し、スタイリングの欠陥を除いてほとんど機能する出力を提供）を観察しました。（出典: Lcsq）</li>
<li>推論効率と量子化: ユーザーは公式のQ4量子化に比べてUnsloth Q2量子化がより良い結果と価値を提供すると比較しています。例えば、Deepseek R1 0528 Q2_Kは250GBで最適な価格性能を提供し、一方でqwen3-235b-a22b-instruct-2507はQ2_K_XLで95GB VRAMで動作し、主観的にはR1 0528と同様に性能を発揮していることを示しています。これは低量子化レベルでのハードウェア効率の大幅な向上を示しています。（出典: -dysangel-）</li>
<li><a href="https://www.reddit.com/gallery/1m7dtpm"><strong>ローカルllm構築、144gb vramモンスター</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/">スコア: 115, コメント: 37</a>): <strong>OPは2x NVIDIA Quadro RTX 8000と1x A6000 GPU（合計144GB VRAM）、AMD Threadripper 7945WX CPU、128GB ECC DDR5-6000 RAMを搭載したカスタムローカルLLMリグを紹介しました。この投稿はハードウェア実装とモデル選択に関する質問を招き、特にGPUが密集して配置されているための潜在的な熱問題とVRAM容量について技術的な焦点を当てています。</strong> 議論では高性能GPUのスタックにおける熱管理とエアフローの重要性が強調され、持続的で安定したLLMワークロードを実現するための適切な冷却の必要性が指摘されています。どのLLMサイズ（例: 70B+）を実行する予定かについての好奇心もあります。</li>
<li>この構築は2x Quadra 8000 GPUと1x A6000を組み合わせ、合計144GB VRAMを提供し、Threadripper 7945wxと128GB ECC DDR5 6000 RAMをペアリングして、非常に大きなまたは複数のLLMをローカルで実行する可能性を提供します。</li>
<li>GPUタイプ（Quadra 8000とA6000）の混合について技術的な議論があり、あるユーザーは異種VRAMプールがLLM推論に影響を与えるかどうかを疑問視しています。特に、最近の大規模モデル（例: Qwen）が48/96GBセットアップで混合される場合の効果的なVRAM利用について懸念が示されています。</li>
<li>高性能構築におけるエアフローと温度管理についての技術的な関心があり、特にGPUが密集して配置されている場合の熱性能について質問があり、持続的で安定したLLMワークロードを実現するための適切な冷却の重要性が強調されています。</li>
</ul>
<h3 id="2-kimi-k2-vs-claude-sonnet-4">2. エージェンティックコーディングモデルの対決: Kimi K2 vs Claude Sonnet 4</h3>
<ul>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/"><strong>Kimi K2 vs Sonnet 4 for Agentic Coding (Tested on Claude Code)</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/">スコア: 104, コメント: 19</a>): <strong>この投稿はMoonshot AIのKimi K2（1Tパラメータ、オープンソース）とAnthropicのClaude Sonnet 4をエージェンティックコーディング、コスト、速度、コーディング/ツール統合性能に焦点を当てて比較しています。Kimi K2は約10倍安価（</strong><code>$0.15/M入力、$2.50/M出力トークン</code><strong> vs. Sonnetの</strong><code>$3/$15</code><strong>）ですが、速度が大幅に遅い（</strong><code>34.1</code><strong> vs. </strong><code>91</code><strong>出力トークン/秒）。両モデルはエージェンティックタスクの完全な実装に苦労していますが、Kimi K2は速度が低いにもかかわらず、プロンプトフォローとエージェンティック流暢性で優れています。デモ付きブログ投稿はこちら:<a href="https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding">Kimi K2 vs. Claude 4 Sonnet for agentic coding</a>。</strong> コメントでは、Kimi K2が指示フォローに優れており、Qwen3-235BやDeepSeek v3を上回り、簡潔で直接的な出力が注目されています。一部のユーザーはIDE統合でのSonnet 4の価格と速度の利点を指摘しています。</li>
<li>あるユーザーはKimi K2が簡潔で非常に指示フォローに優れた出力を提供し、コーディングタスクでユーザーの意図をフォローする点でQwen3-235bやDeepSeek v3を上回っていると観察しています。ただし、クローズドモデルの使用が限られているため、ClaudeやSonnetとの直接比較はありません。</li>
<li>対照的な経験として、あるユーザーはClaude CodeでKimi K2を使用した際に、K2のコードがしばしばコンパイルに失敗し、意図に一致せず、不適切に新しいファイルを作成する代わりに編集を行うと報告しています。一方でClaudeはタスクを確実に処理し、MoonshotのAPI速度が欠点として指摘されています。</li>
<li>議論では、プロンプトキャッシングが入力トークンコストを削減し、Sonnet 4のリスト価格が他のモデルと比較して高い場合でも、エージェンティックコーディングタスクにおいてより費用対効果が高い可能性があることが指摘されています。</li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/"><strong>Qwen 3 Coderは私のテストではかなり良いです</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/">スコア: 180, コメント: 37</a>): <strong>ユーザーはQwen 3 Coder（OpenRouter経由、Alibabaクラウド推論、約60トークン/秒）を使用して、以前にKimi K2（Groq Q4）やClaude Sonnetで試みた半複雑なWeb ACL統合を高コンテキスト（1200行のアーキテクチャ、約30kプロンプトトークン）シナリオでテストしました。Qwen 3 Coderは信頼性が高く（「タスクを一発で完了」）、修正が必要なく、Kimi K2 Q4を上回り、このコンテキストではSonnet 4と同等と見なされました—オープンソースコードモデルの大きな進歩を示しています。主な欠点として、OpenRouter経由での推論コストが高い（1つの機能タスクで5ドル）ことが挙げられ、サブスクリプションLLM（例: Claude Pro/Sonnet 4月額）に比べてオープンモデルの使用可能性拡大に懸念が生じています。</strong> コメントでは、高いオープンモデル価格が競争の欠如、モデルサイズ/メモリ要件、プロバイダー側の補助金の欠如（AnthropicのClaudeスタックとは異なる）によるものであることが指摘されています。あるユーザーはACLセキュリティ原則の変更（デフォルト拒否アプローチ）を提案し、より堅牢なLLM駆動のコーディング結果を得ることを提案しています。</li>
<li>Qwen 3 Coder（OpenRouter経由）とClaude Codeの価格差は、Qwen 3の最近のローンチ（競争が始まる前にプロバイダーが高価格を設定できる）、その大きなモデルサイズが高いメモリ要求を引き起こすこと、そしてAnthropicが独自のスタックと資本を利用してClaude推論を補助できるという事実によるものとされています。補助金が消えると、価格差は縮小する可能性があります。（出典: md5nake）</li>
<li>技術的性能: ユーザーはMoonshot経由でAnthropicエンドポイントを使用すると高いキャッシュヒット率（約80%のトークンがキャッシュから提供される）を得られることを指摘し、実際のコストがリスト価格よりもはるかに低くなることを観察しました—「Claude Codeで25ドルと表示されていたのに実際には2ドルしかかかりませんでした」。彼らは強力なコーディング性能（約5k LOCを一度に処理し、スタイリングの欠陥を除いてほとんど機能する出力を提供）を観察しました。（出典: Lcsq）</li>
<li>推論効率と量子化: ユーザーは公式のQ4量子化に比べてUnsloth Q2量子化がより良い結果と価値を提供すると比較しています。例えば、Deepseek R1 0528 Q2_Kは250GBで最適な価格性能を提供し、一方でqwen3-235b-a22b-instruct-2507はQ2_K_XLで95GB VRAMで動作し、主観的にはR1 0528と同様に性能を発揮していることを示しています。これは低量子化レベルでのハードウェア効率の大幅な向上を示しています。（出典: -dysangel-）</li>
</ul>
<h3 id="3-aillm">3. オープンソースAIとLLMアーキテクチャに関する政府および業界のイニシアティブ</h3>
<ul>
<li><a href="https://i.redd.it/736cx17efnef1.png"><strong>「オープンソースとオープンウェイトAI」の奨励が米国政府の公式政策となりました。</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/">スコア: 536, コメント: 141</a>): <strong>この画像は「オープンソースとオープンウェイトAI」を支持し奨励する戦略を明確にした米国政府の公式政策文書のスクリーンショットまたは抜粋です。この政策は、スタートアップ、企業、研究者にとっての加速されたイノベーション、透明性の向上、費用対効果の高いアクセスなどの実用的な利点を強調しています。特に、学術機関や小規模企業が最先端モデルの開発と展開における主要な障壁に直接対処するための大規模計算インフラへのアクセスを促進することを提案しています。全文は<a href="https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf">ホワイトハウスの出版物</a>で利用可能です。</strong> 注目すべきコメントでは、この政策の健全な市場効果が観察され、オープンソースAIの競争がさらなるイノベーションと社会的利益を引き起こし、個々の企業が文化的にあまり投資していない場合でも国家利益と一致する可能性があることが強調されています。</li>
<li>ArtArtArt123456は重要な変化を指摘しています。政府だけでなく民間企業も、オープンソースとオープンウェイトAIを戦略的な文化的および社会的資産として認識しており、これが世論（「プロパガンダ、マインドシェア」）に影響を与える可能性があることを示しています。これはAI競争が市場主導のイノベーションから国家的影響と公共感情の問題に拡大していることを示しています。</li>
<li>Recoil42は、オープンソースLLMの米国政策認識がプロパガンダへの有用性を明示的に言及していることを指摘し、LLMの二重用途の可能性を公式に認識していることを示しています。この意味するところは、政策と規制の焦点が商業的または技術的なものだけでなく、AIの広範な社会的影響をますます考慮するようになることです。</li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"><strong>Google DeepMindがMixture-of-Recursionsをリリース</strong></a> (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/">スコア: 192, コメント: 29</a>): <strong>Google DeepMindは、LLMのための高度なTransformerアーキテクチャであるMixture-of-Recursionsを導入しました。このアーキテクチャでは、トークンごとに選択的かつ動的に適用される再帰的Transformerモジュールを使用し、単一のフォワードパス内で異なるトークンが異なる数の変換ステップ（再帰）を受けることを可能にします。このアプローチは、従来のTransformerとは異なり、トークンごとに異なる計算深度を可能にし、効率性とスケーラビリティを向上させるとされています。技術的なビデオ説明は<a href="https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR">こちら</a>、ブログの要約は<a href="https://medium.com/data-science-in-your-pocket/googles-mixture-of-recursions-end-of-transformers-b8de0fe9c83b">こちら</a>で確認できます。</strong> コメントでは、Transformer内での自己混合やインシチュー層再利用との類似性が指摘されていますが、Mixture-of-Recursionsがよりスケーラブルで構造的制限が少ない可能性があると述べられています。</li>
<li>コメントでは、Mixture-of-Recursionsアプローチが比較的小さなモデル（最大1.7Bパラメータ）でのみ検証されていることが指摘されており、これらの発見がまだスケールで証明されていないことを示唆しています。</li>
<li>別のユーザーは、標準的なTransformer内での自己混合（層が再帰的に使用されるか、パススルーメカニズムを通じてマージされる）と概念的に比較し、この新しい方法がよりスケーラブルで自己混合アーキテクチャよりも不安定性が少ないと述べています。</li>
<li>ユーザーは、同等の計算コストでこのアプローチが大幅な生の性能向上をもたらさない可能性があり、これが大規模な組織による展開よりもローカルアプリケーションにとって魅力的になる可能性があると推測しています。</li>
</ul>
<hr />
<p>以下のセクションは翻訳対象外です。</p>
    </main>

    <footer>
        <p>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
            <a href="https://news.smol.ai/">news.smol.ai</a>
        </p>
    </footer>
</body>
</html>