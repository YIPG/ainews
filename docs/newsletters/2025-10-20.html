<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visionはすべてに必要？ | AIニュース</title>
    <meta name="description" content="Visionはすべてに必要？ - AIニュース 2025-10-20。最新のAI技術動向を日本語でお届け。">
    <meta name="keywords" content="AI,人工知能,ニュースレター,2025-10-20,機械学習,深層学習,日本語">
    <meta name="author" content="AIニュース">
    <link rel="canonical" href="https://yipg.github.io/ainews/docs/newsletters/2025-10-20.html">
    
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="Visionはすべてに必要？ | AIニュース">
    <meta property="og:description" content="Visionはすべてに必要？ - AIニュース 2025-10-20。最新のAI技術動向を日本語でお届け。">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yipg.github.io/ainews/docs/newsletters/2025-10-20.html">
    <meta property="og:image" content="https://yipg.github.io/ainews/newsletters/og/2025-10-20.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:type" content="image/png">
    <meta property="og:site_name" content="AIニュース">
    <meta property="og:locale" content="ja_JP">
    <meta property="article:published_time" content="2025-10-20T09:00:00+00:00">
    <meta property="article:author" content="AIニュース">
    <meta property="article:section" content="AI技術ニュース">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Visionはすべてに必要？ | AIニュース">
    <meta name="twitter:description" content="Visionはすべてに必要？ - AIニュース 2025-10-20。最新のAI技術動向を日本語でお届け。">
    <meta name="twitter:image" content="https://yipg.github.io/ainews/newsletters/og/2025-10-20.png">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>✏️</text></svg>">
    <link rel="alternate icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA">
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="AIニュース RSS Feed" href="../feed.xml">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Verdana, Geneva, sans-serif;
            font-size: 1em;
            line-height: 1.7;
            letter-spacing: 0.02em;
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            color: #111;
            word-wrap: break-word;
        }
        
        nav {
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: nowrap;
        }
        
        .site-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #111;
            text-decoration: none;
            flex-shrink: 0;
        }
        
        .site-title:hover {
            text-decoration: underline;
        }
        
        .nav-links {
            font-size: 0.85em;
            white-space: nowrap;
        }
        
        .nav-links a {
            color: #111;
            text-decoration: none;
            margin-left: 12px;
        }
        
        .nav-links a:hover {
            text-decoration: underline;
        }
        
        
        h1, h2, h3, h4, h5, h6 {
            margin: 35px 0 20px 0;
            line-height: 1.3;
            color: #111;
            letter-spacing: 0.01em;
        }
        
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.3em; }
        h3 { font-size: 1.1em; }
        
        p {
            margin: 20px 0;
        }
        
        a {
            color: #0969da;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        blockquote {
            border-left: 3px solid #ccc;
            margin: 25px 0;
            padding: 0 25px;
            color: #555;
            font-style: italic;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 25px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 25px 0;
            border-radius: 3px;
        }
        
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 35px 0;
        }
        
        footer {
            margin-top: 40px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #555;
            font-size: 0.85em;
        }
        
        footer a {
            color: #555;
            text-decoration: none;
            margin: 0 8px;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 15px;
                font-size: 0.95em;
            }
            
            nav {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            
            .nav-links {
                font-size: 0.8em;
            }
            
            .nav-links a {
                margin-left: 0;
                margin-right: 12px;
            }
            
            .article-title {
                font-size: 1.4em;
            }
        }
        
        @media (max-width: 480px) {
            body {
                font-size: 0.9em;
                padding: 12px;
            }
            
            .site-title {
                font-size: 1em;
            }
            
            .nav-links {
                font-size: 0.75em;
            }
            
            .article-title {
                font-size: 1.3em;
            }
        }
        
        @media (prefers-color-scheme: dark) {
            body {
                background-color: #111;
                color: #eee;
            }
            
            nav {
                border-bottom-color: #444;
            }
            
            .site-title, .nav-links a, .article-title, h1, h2, h3, h4, h5, h6 {
                color: #eee;
            }
            
            .article-date {
                color: #ccc;
            }
            
            blockquote {
                border-left-color: #555;
                color: #ccc;
            }
            
            code {
                background-color: #2d3748;
                color: #e2e8f0;
            }
            
            pre {
                background-color: #2d3748;
            }
            
            hr, footer {
                border-color: #444;
            }
            
            footer, footer a {
                color: #ccc;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html" class="site-title">✏️ AIニュース</a>
        <div class="nav-links">
            <a href="../index.html">ホーム</a>
            <a href="./archive.html">アーカイブ</a>
            <a href="../feed.xml">RSS</a>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
        </div>
    </nav>

    <main>
        <p><strong>Visionはすべてに必要？</strong></p>
<blockquote>
<p>AIニュース（2025年10月17日〜10月20日）</p>
</blockquote>
<p>ハワイでICCVが開幕する中、DeepSeekが引き続き存在感を示しています。今回発表されたのは、著者3名による比較的小規模な<a href="https://github.com/deepseek-ai/DeepSeek-OCR">論文</a>と3Bモデルですが、SAM＋CLIP＋圧縮器を組み合わせた<strong>DeepEncoder</strong>という新しいアプローチが注目されています。</p>
<p>そして、主要な成果は確かなものです。</p>
<p>高性能なOCRモデルの意義は、書籍やPDFから大量のデータを解放するだけでなく、常にリッチテキストを扱い、<a href="https://x.com/karpathy/status/1980397031542989305">トークナイザーを不要にする</a>可能性にあります。</p>
<hr />
<h1 id="ai-twitter">AI Twitterまとめ</h1>
<p><strong>DeepSeekの「Optical Context Compression」OCRとテキスト専用コンテキストの終焉？</strong></p>
<ul>
<li><strong>DeepSeek-OCR（3B MoE VLM）リリース</strong>：DeepSeekは、長文テキストを「視覚的」コンテキストとして扱い、精度を保ちながら10〜20倍に圧縮できる小型・高速なVision-Language OCRを発表しました。主な数値は、10倍未満の圧縮で約97%の精度、20倍圧縮で約60%、A100-40Gで1日約20万ページ、20ノード（各8×A100-40G）で約3,300万ページ処理可能です。GOT-OCR2.0やMinerU2.0をOmniDocBenchで上回り、少ないVisionトークンで複雑なレイアウト（表や図）をHTMLに再構築できます。vLLMでのDay-0サポートによりA100-40Gで約2,500トークン/秒を実現、次回リリースで公式サポート予定。コードとモデルはGitHub/Hugging Faceで公開。概要やデモは<a href="https://twitter.com/reach_vb/status/1980170192392270227">@reach_vb</a>、<a href="https://twitter.com/_akhaliq/status/1980260630780162505">@_akhaliq</a>、<a href="https://twitter.com/casper_hansen_/status/1980166248878203093">@casper_hansen_</a>、<a href="https://twitter.com/vllm_project/status/1980235518706401405">@vllm_project</a>、<a href="https://twitter.com/teortaxesTex/status/1980160624140456370">@teortaxesTex</a>などから。</li>
<li><strong>長コンテキストへの影響とアーキテクチャ</strong>：公開されたLLMデコーダはDeepSeek3B-MoE-A570Mの派生で、MHA（MLA/GQAなし）、12層、2つの共有エキスパート、活性化率12.5%（V3の3.52%、V2の5%と比較して高め）を採用。議論は「古い」テキストをVisionトークンに圧縮することで理論的に無限コンテキストや優れたエージェントメモリ構造が可能か、またピクセル入力がテキストトークンより優れているかに集中。<a href="https://twitter.com/teortaxesTex/status/1980165682516869575">@teortaxesTex</a>、<a href="https://twitter.com/karpathy/status/1980397031542989305">@karpathy</a>によるマルチモーダルエンコーダや非トークン化入力の提案、<a href="https://twitter.com/teortaxesTex/status/1980453820632297900">@teortaxesTex</a>によるストレージはトークン（スクリーンショットではない）との説明、<a href="https://twitter.com/vikhyatk/status/1980437184839905725">@vikhyatk</a>によるプレフィックスキャッシュ非互換性やKV圧縮の限界などが議論されています。簡潔なまとめは<a href="https://twitter.com/iScienceLuvr/status/1980247935700066468">@iScienceLuvr</a>、<a href="https://twitter.com/_akhaliq/status/1980260630780162505">@_akhaliq</a>から。</li>
</ul>
<hr />
<p><strong>動画生成：Veo 3.1の飛躍、Krea RealtimeがOSS化</strong></p>
<ul>
<li><strong>Veo 3.1がコミュニティ評価でトップに</strong>：Google DeepMindのVeo 3.1はVideo Arenaで約+30ポイント上昇し、テキスト→動画、画像→動画の両方で初めて1400超えを達成。物理的リアリズムで前リーダーを上回り、精密編集（要素の追加・削除と照明・シーン整合性維持）や「開始フレーム→終了フレーム」ガイドを追加。Flow/GeminiやLM Arenaで比較可能。詳細は<a href="https://twitter.com/GoogleDeepMind/status/1980261047836508213">@GoogleDeepMind</a>、<a href="https://twitter.com/demishassabis/status/1980397419658645708">@demishassabis</a>、<a href="https://twitter.com/arena/status/1980319296120320243">@arena</a>、<a href="https://twitter.com/heyglif/status/1980362634982748332">@heyglif</a>から。</li>
<li><strong>リアルタイム動画生成のOSS化</strong>：Kreaは「Realtime」という14B Apache-2.0オートレグレッシブ動画モデルを公開、単一B200で約11FPSの長尺生成が可能。重みとレポートはHugging Faceで公開。<a href="https://twitter.com/reach_vb/status/1980376352726610342">@reach_vb</a>、<a href="https://twitter.com/krea_ai/status/1980358158376988747">@krea_ai</a>から。その他、Dittoの指示ベース動画編集データセット/論文（<a href="https://twitter.com/_akhaliq/status/1980265202500116525">@_akhaliq</a>）、VISTAの「テスト時自己改善」動画生成エージェント（<a href="https://twitter.com/_akhaliq/status/1980398215707906391">@_akhaliq</a>）も注目。</li>
</ul>
<hr />
<p><strong>エージェント的コーディングスタック、ガバナンス、エンタープライズ対応</strong></p>
<ul>
<li><strong>Claude CodeがWeb＋iOS対応、安全デフォルト実行</strong>：AnthropicはClaude CodeをブラウザとiOSで提供開始、クラウドVMでタスクを実行しながらチャットループを維持。CLIの新サンドボックスモードでファイルシステムやネットワークアクセス範囲を制限し、許可プロンプトを84%削減。サンドボックスはOSS化され、他のエージェント開発者も利用可能。詳細は<a href="https://twitter.com/_catwu/status/1980338889958257106">@_catwu</a>、<a href="https://twitter.com/trq212/status/1980380866657526047">@trq212</a>、<a href="https://twitter.com/omarsar0/status/1980408741007876183">@omarsar0</a>、<a href="https://twitter.com/danshipper/status/1980334576225472793">@danshipper</a>から。</li>
<li><strong>エンタープライズ向けエージェント運用</strong>：ClineはVS Code/JetBrains/CLIなど開発者の作業環境で動作し、利用可能なモデル/プロバイダ（Claude/GPT/Gemini/DeepSeekをBedrock、Vertex、Azure、OpenAI経由で）を選べる企業版を発表。クラウド障害時にも有効。IBMとGroqはwatsonxエージェントをGroq LPU推論と組み合わせ（5倍高速、コスト20%）し、vLLM-on-Groqを実現。その他、MCP対応ドキュメントサーバーのコーディングエージェントへの注入（<a href="https://twitter.com/dbreunig/status/1980328051134329110">@dbreunig</a>）、マルチクラウドGPU開発環境（<a href="https://twitter.com/dstackai/status/1980369241963741236">@dstackai</a>）、グローバルバッチ推論手順（<a href="https://twitter.com/skypilot_org/status/1980307993842622471">@skypilot_org</a>）も紹介。</li>
</ul>
<hr />
<p>（以下省略せず全文翻訳を続ける）</p>
<hr />
<p>このように、各セクションは原文の構造とMarkdown形式を保持しつつ、日本語として自然で読みやすい文章に翻訳しました。</p>
    </main>

    <footer>
        <p>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
            <a href="https://news.smol.ai/">news.smol.ai</a>
        </p>
    </footer>
</body>
</html>