<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>静かな一日 | AIニュース</title>
    <meta name="description" content="静かな一日 - AIニュース 2026-01-19。最新のAI技術動向を日本語でお届け。">
    <meta name="keywords" content="AI,人工知能,ニュースレター,2026-01-19,機械学習,深層学習,日本語">
    <meta name="author" content="AIニュース">
    <link rel="canonical" href="https://yipg.github.io/ainews/docs/newsletters/2026-01-19.html">
    
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="静かな一日 | AIニュース">
    <meta property="og:description" content="静かな一日 - AIニュース 2026-01-19。最新のAI技術動向を日本語でお届け。">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yipg.github.io/ainews/docs/newsletters/2026-01-19.html">
    <meta property="og:image" content="https://yipg.github.io/ainews/newsletters/og/2026-01-19.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:type" content="image/png">
    <meta property="og:site_name" content="AIニュース">
    <meta property="og:locale" content="ja_JP">
    <meta property="article:published_time" content="2026-01-19T09:00:00+00:00">
    <meta property="article:author" content="AIニュース">
    <meta property="article:section" content="AI技術ニュース">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="静かな一日 | AIニュース">
    <meta name="twitter:description" content="静かな一日 - AIニュース 2026-01-19。最新のAI技術動向を日本語でお届け。">
    <meta name="twitter:image" content="https://yipg.github.io/ainews/newsletters/og/2026-01-19.png">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>✏️</text></svg>">
    <link rel="alternate icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA">
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="AIニュース RSS Feed" href="../feed.xml">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Verdana, Geneva, sans-serif;
            font-size: 1em;
            line-height: 1.7;
            letter-spacing: 0.02em;
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            color: #111;
            word-wrap: break-word;
        }
        
        nav {
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: nowrap;
        }
        
        .site-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #111;
            text-decoration: none;
            flex-shrink: 0;
        }
        
        .site-title:hover {
            text-decoration: underline;
        }
        
        .nav-links {
            font-size: 0.85em;
            white-space: nowrap;
        }
        
        .nav-links a {
            color: #111;
            text-decoration: none;
            margin-left: 12px;
        }
        
        .nav-links a:hover {
            text-decoration: underline;
        }
        
        
        h1, h2, h3, h4, h5, h6 {
            margin: 35px 0 20px 0;
            line-height: 1.3;
            color: #111;
            letter-spacing: 0.01em;
        }
        
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.3em; }
        h3 { font-size: 1.1em; }
        
        p {
            margin: 20px 0;
        }
        
        a {
            color: #0969da;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        blockquote {
            border-left: 3px solid #ccc;
            margin: 25px 0;
            padding: 0 25px;
            color: #555;
            font-style: italic;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 25px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 25px 0;
            border-radius: 3px;
        }
        
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 35px 0;
        }
        
        footer {
            margin-top: 40px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #555;
            font-size: 0.85em;
        }
        
        footer a {
            color: #555;
            text-decoration: none;
            margin: 0 8px;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 15px;
                font-size: 0.95em;
            }
            
            nav {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            
            .nav-links {
                font-size: 0.8em;
            }
            
            .nav-links a {
                margin-left: 0;
                margin-right: 12px;
            }
            
            .article-title {
                font-size: 1.4em;
            }
        }
        
        @media (max-width: 480px) {
            body {
                font-size: 0.9em;
                padding: 12px;
            }
            
            .site-title {
                font-size: 1em;
            }
            
            .nav-links {
                font-size: 0.75em;
            }
            
            .article-title {
                font-size: 1.3em;
            }
        }
        
        @media (prefers-color-scheme: dark) {
            body {
                background-color: #111;
                color: #eee;
            }
            
            nav {
                border-bottom-color: #444;
            }
            
            .site-title, .nav-links a, .article-title, h1, h2, h3, h4, h5, h6 {
                color: #eee;
            }
            
            .article-date {
                color: #ccc;
            }
            
            blockquote {
                border-left-color: #555;
                color: #ccc;
            }
            
            code {
                background-color: #2d3748;
                color: #e2e8f0;
            }
            
            pre {
                background-color: #2d3748;
            }
            
            hr, footer {
                border-color: #444;
            }
            
            footer, footer a {
                color: #ccc;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html" class="site-title">✏️ AIニュース</a>
        <div class="nav-links">
            <a href="../index.html">ホーム</a>
            <a href="./archive.html">アーカイブ</a>
            <a href="../feed.xml">RSS</a>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
        </div>
    </nav>

    <main>
        <p><strong>静かな一日</strong></p>
<blockquote>
<p>AIニュース（2026年1月16日〜1月19日）</p>
</blockquote>
<p>時間があれば、<a href="https://x.com/arcprize/status/2013369761250582794?s=46">ARC AGI 2025 Report</a>をご覧になることをおすすめします。</p>
<hr />
<h1 id="ai-twitter-recap">AI Twitter Recap</h1>
<p><strong>「メモリ」とコンテキストをスケーリングする新しいアーキテクチャ</strong></p>
<ul>
<li><strong>STEM（Scaling Transformers with Embedding Modules）</strong>：Carnegie MellonとMetaによるアプローチで、MoE型の動的ルーティングを使わずにTransformerの<strong>パラメトリックメモリ</strong>をスケーリングします。ポイントは、FFNのアッププロジェクションの約1/3を削除し、<strong>トークンインデックス化された埋め込みルックアップ</strong>に置き換えることです。ゲートとダウンプロジェクションは密なまま維持します。ルックアップが静的なため、実行時のルーティングのオーバーヘッドや不安定性を回避でき、<strong>CPUオフロード＋非同期プリフェッチ</strong>も可能になります。これにより、<strong>モデル容量をトークンごとのFLOPsやデバイス間通信から切り離す</strong>ことができます（<a href="https://twitter.com/TheTuringPost/status/2013011864880660495">概要</a>、<a href="https://twitter.com/TheTuringPost/status/2013011880210731167">ステップごとの説明</a>、<a href="https://twitter.com/TheTuringPost/status/2013011892672086377">MoEが非効率になり得る理由</a>）。<ul>
<li>実務的ポイント：「疎容量」は必ずしもMoEルーター＋エキスパート並列を意味するわけではなく、静的スパース化は<strong>システムに優しい</strong>（予測可能なアクセスパターン、通信量低減）。</li>
</ul>
</li>
<li><strong>RePo（Context Re-Positioning） from Sakana AI</strong>：LMが<strong>コンテンツの関連性に基づいて位置構造を並べ替える</strong>軽量モジュール。これにより、関連性の高い遠方の項目を「近くに引き寄せ」、ノイズを遠ざけることができます。固定トークンインデックスがモデルに非構造化入力の処理負荷を強いるという認知負荷理論の観点から説明されます。RePoは<strong>ノイズの多いコンテキスト、構造化データ、長距離依存関係</strong>に対する堅牢性を狙っています（<a href="https://twitter.com/SakanaAILabs/status/2013046887746843001">発表</a>、<a href="https://twitter.com/SakanaAILabs/status/2013232698672742472">コード</a>、<a href="https://twitter.com/SakanaAILabs/status/2013232698672742472">リポジトリリンク</a>）。<ul>
<li>実務的ポイント：検索やパッキングの工夫を補完するもので、RePoは単なる検索改善ではなく<strong>適応的な順序付け</strong>のためのアーキテクチャ的調整です。</li>
</ul>
</li>
</ul>
<p><strong>モデルリリース：GLM-4.7-Flashと「MLA＋小規模MoE」の潮流</strong></p>
<ul>
<li><strong>Zhipu AI GLM-4.7-Flash</strong>：<strong>30Bクラスのローカルコーディング／エージェントモデル</strong>として公開され、軽量かつ展開しやすいと位置付けられています。Zhipuはこれを「30Bクラスの新しい標準」と呼び、<strong>コーディング＋エージェント用途</strong>、翻訳、長文コンテキスト、創作に推奨しています（<a href="https://twitter.com/Zai_org/status/2013261304060866758">ローンチ</a>、<a href="https://twitter.com/louszbd/status/2013262379874693155">開発者コメント</a>）。後にZhipuは<strong>GLM-4.7-Flashが30B-A3B MoEモデル</strong>であると明言しました（<a href="https://twitter.com/Zai_org/status/2013280523871752319">仕様</a>）。<ul>
<li>コミュニティやアナリストはアーキテクチャの変化に注目しています。GLMは<strong>MLA</strong>に切り替え、ダウンプロジェクション後のヘッド次元やヘッド数を増やすなど、QwenやDeepSeekに見られる設計傾向を踏襲しています（<a href="https://twitter.com/stochasticchasm/status/2013268543064715629">stochasticchasm</a>、<a href="https://twitter.com/eliebakouch/status/2013272478018048209">eliebakouch</a>）。別のまとめでは、トークンごとに約<strong>3Bアクティブ</strong>で、<strong>SWE-bench Verified</strong>、τ²-Bench、HLE、BrowseCompで強いベンチマーク結果を示し、<strong>LCB</strong>ではQwenが優位としています（<a href="https://twitter.com/gm8xx8/status/2013310047770599448">gm8xx8</a>）。モデルカードでの確認を推奨します。</li>
</ul>
</li>
<li><strong>「圧縮」ナラティブ</strong>：GLMの進化を「より大きなモデルを小型化する」流れとして捉えるコメントもあり（例：「GLM-4.5 110B → GLM-4.7 31B」）、今後の<strong>GLM-4.7V</strong>とQwen3-VLの比較が注目されています（<a href="https://twitter.com/casper_hansen_/status/2013294519546978719">casper_hansen_</a>）。これは確定した学習レシピではなく解釈的な見方です。</li>
<li><strong>小型モデルのツーリングでの復権</strong>：エンジニアが<strong>速度／レイテンシ</strong>と「十分な」知能を同期的コーディングに優先する傾向が複数の投稿で見られます。これにより、95%以上のインタラクティブタスクでは大型モデルの限界効用が低下し、<strong>高速推論でフロンティア級品質</strong>を目指す方向にシフトしています（<a href="https://twitter.com/amanrsanger/status/2013387140537950715">amanrsanger</a>）。</li>
</ul>
<p><strong>推論・展開インフラ：ローカルランタイム、vLLM/MLX、そして「フルスタック」システム論文</strong></p>
<ul>
<li><strong>GLM-4.7-FlashのDay-0エコシステム対応</strong>：<ul>
<li><strong>mlx-lm</strong>：GLM 4.7 Flashが<strong>mlx-lm 0.30.3</strong>でサポートされ、M5 32GBラップトップでの4ビット性能は生成速度約<strong>43 tok/s</strong>、プリフィル約<strong>800 tok/s</strong>と報告されています（<a href="https://twitter.com/awnihannun/status/2013286079470645353">awnihannun</a>）。後のmlx-lmリリースノートでは、継続的バッチ処理／分散改善、autoAWQ/autoGPTQ対応が記載されています（<a href="https://twitter.com/awnihannun/status/2013316769163751662">awnihannun</a>）。</li>
<li><strong>LM Studio</strong>：GLM-4.7-Flashが<strong>Apple Silicon向けMLX</strong>経由でMac上の<strong>30Bローカルコーディングエージェント</strong>として利用可能です（<a href="https://twitter.com/lmstudio/status/2013339758139789389">lmstudio</a>）。</li>
<li><strong>Ollama</strong>：GLM-4.7-Flashが<strong>Ollama v0.14.3+（プレリリース）</strong>で利用可能です（<a href="https://twitter.com/ollama/status/2013372316021834086">ollama</a>）。</li>
<li><strong>vLLM</strong>：vLLMプロジェクトが「Day-0対応」PRを発表しました（<a href="https://twitter.com/vllm_project/status/2013421647215407587">vllm_project</a>）。</li>
<li><strong>opencode＋HF推論プロバイダ</strong>：GLM-4.7-FlashがHugging Face Inference Providers経由でOpenCodeに統合されました（<a href="https://twitter.com/victormustar/status/2013297272025424120">victormustar</a>）。Ollama＋HarborでローカルGLM-4.7-Flashを動かす例もあります（<a href="https://twitter.com/Everlier/status/2013383690756276454">Everlier</a>）。</li>
</ul>
</li>
<li><strong>Huawei／中国の推論システム「2025旗艦作」まとめ</strong>（Zhihu投稿者による要約）：KVキャッシュ容量の壁、PD分割／統合利用、ハイブリッドスケジューリング、キャッシュアフィニティ／負荷分散、KVCache中心のエージェントメモリなどを狙ったシステム設計案が密集しています。特筆すべきは「冷たい」KVをDRAMにオフロード、「デコードアテンションをプリフィルGPUに流す」、「レイテンシの余裕を資源として活用」、「二重ハッシュルーティング（two choicesの力）」、「エージェントメモリを再利用可能なKVブロックとしてプレフィックスの継続性とキャッシュを保持」などです（<a href="https://twitter.com/ZhihuFrontier/status/2013127635589800172">ZhihuFrontier</a>）。<ul>
<li>実務的ポイント：重心は個別カーネルから<strong>エンドツーエンドのSLOグッドプット</strong>設計へ移行しています。</li>
</ul>
</li>
<li><strong>Cerebras vs GPUのトレードオフ</strong>：あるスレッドでは「コンピュータアーキテクチャにおいて無料はない」と強調しています。Cerebrasは帯域幅／レイテンシを得る代わりに、GPU向けワークロードでのFLOPs／メモリ効率を犠牲にしますが、他では難しい超低レイテンシの小型モデルケースを可能にします（<a href="https://twitter.com/itsclivetime/status/2013084127218852207">itsclivetime</a>）。関連する推測として、「Codex on Cerebras」がエージェントハーネスの期待値を再設定する可能性が指摘されています（<a href="https://twitter.com/dbreunig/status/2013285271438311608">dbreunig</a>）。</li>
</ul>
<hr />
<p>（以下省略。続きは同様の翻訳スタイルで）</p>
    </main>

    <footer>
        <p>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
            <a href="https://news.smol.ai/">news.smol.ai</a>
        </p>
    </footer>
</body>
</html>