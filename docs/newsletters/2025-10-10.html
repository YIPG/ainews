<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>静かな一日 | AIニュース</title>
    <meta name="description" content="静かな一日 - AIニュース 2025-10-10。最新のAI技術動向を日本語でお届け。">
    <meta name="keywords" content="AI,人工知能,ニュースレター,2025-10-10,機械学習,深層学習,日本語">
    <meta name="author" content="AIニュース">
    <link rel="canonical" href="https://yipg.github.io/ainews/docs/newsletters/2025-10-10.html">
    
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="静かな一日 | AIニュース">
    <meta property="og:description" content="静かな一日 - AIニュース 2025-10-10。最新のAI技術動向を日本語でお届け。">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yipg.github.io/ainews/docs/newsletters/2025-10-10.html">
    <meta property="og:image" content="https://yipg.github.io/ainews/newsletters/og/2025-10-10.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:type" content="image/png">
    <meta property="og:site_name" content="AIニュース">
    <meta property="og:locale" content="ja_JP">
    <meta property="article:published_time" content="2025-10-10T09:00:00+00:00">
    <meta property="article:author" content="AIニュース">
    <meta property="article:section" content="AI技術ニュース">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="静かな一日 | AIニュース">
    <meta name="twitter:description" content="静かな一日 - AIニュース 2025-10-10。最新のAI技術動向を日本語でお届け。">
    <meta name="twitter:image" content="https://yipg.github.io/ainews/newsletters/og/2025-10-10.png">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>✏️</text></svg>">
    <link rel="alternate icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA">
    
    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="AIニュース RSS Feed" href="../feed.xml">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Verdana, Geneva, sans-serif;
            font-size: 1em;
            line-height: 1.7;
            letter-spacing: 0.02em;
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            color: #111;
            word-wrap: break-word;
        }
        
        nav {
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            justify-content: space-between;
            flex-wrap: nowrap;
        }
        
        .site-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #111;
            text-decoration: none;
            flex-shrink: 0;
        }
        
        .site-title:hover {
            text-decoration: underline;
        }
        
        .nav-links {
            font-size: 0.85em;
            white-space: nowrap;
        }
        
        .nav-links a {
            color: #111;
            text-decoration: none;
            margin-left: 12px;
        }
        
        .nav-links a:hover {
            text-decoration: underline;
        }
        
        
        h1, h2, h3, h4, h5, h6 {
            margin: 35px 0 20px 0;
            line-height: 1.3;
            color: #111;
            letter-spacing: 0.01em;
        }
        
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.3em; }
        h3 { font-size: 1.1em; }
        
        p {
            margin: 20px 0;
        }
        
        a {
            color: #0969da;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin: 20px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
        }
        
        blockquote {
            border-left: 3px solid #ccc;
            margin: 25px 0;
            padding: 0 25px;
            color: #555;
            font-style: italic;
        }
        
        code {
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 25px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 25px 0;
            border-radius: 3px;
        }
        
        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 35px 0;
        }
        
        footer {
            margin-top: 40px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #555;
            font-size: 0.85em;
        }
        
        footer a {
            color: #555;
            text-decoration: none;
            margin: 0 8px;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 15px;
                font-size: 0.95em;
            }
            
            nav {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            
            .nav-links {
                font-size: 0.8em;
            }
            
            .nav-links a {
                margin-left: 0;
                margin-right: 12px;
            }
            
            .article-title {
                font-size: 1.4em;
            }
        }
        
        @media (max-width: 480px) {
            body {
                font-size: 0.9em;
                padding: 12px;
            }
            
            .site-title {
                font-size: 1em;
            }
            
            .nav-links {
                font-size: 0.75em;
            }
            
            .article-title {
                font-size: 1.3em;
            }
        }
        
        @media (prefers-color-scheme: dark) {
            body {
                background-color: #111;
                color: #eee;
            }
            
            nav {
                border-bottom-color: #444;
            }
            
            .site-title, .nav-links a, .article-title, h1, h2, h3, h4, h5, h6 {
                color: #eee;
            }
            
            .article-date {
                color: #ccc;
            }
            
            blockquote {
                border-left-color: #555;
                color: #ccc;
            }
            
            code {
                background-color: #2d3748;
                color: #e2e8f0;
            }
            
            pre {
                background-color: #2d3748;
            }
            
            hr, footer {
                border-color: #444;
            }
            
            footer, footer a {
                color: #ccc;
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="../index.html" class="site-title">✏️ AIニュース</a>
        <div class="nav-links">
            <a href="../index.html">ホーム</a>
            <a href="./archive.html">アーカイブ</a>
            <a href="../feed.xml">RSS</a>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
        </div>
    </nav>

    <main>
        <p><strong>静かな一日</strong></p>
<blockquote>
<p>AIニュース（2025年10月9日〜10月10日）</p>
</blockquote>
<p>AIE CODEの<a href="https://apply.ai.engineer/"><strong>第2回応募</strong></a>はあと5日で締め切りです。</p>
<hr />
<h1 id="ai-twitter">AI Twitterまとめ</h1>
<p><strong>推論：FrontierMath対決、Markovian Thinking、そして「推論トレーニング」が実際に教えること</strong></p>
<ul>
<li>FrontierMath Tier 4の結果：計算資源を大量に使う設定で、GPT‑5 Proが13%の正答率で新記録を達成し、Gemini 2.5 Deep Thinkをわずか1問差で上回りました（統計的有意差はなし）。Grok 4 Heavyは遅れを取っています。Epochはリーク懸念について説明し、OpenAIが48問中28問にアクセスしていること、GPT‑5 Proの8問中5問が保持セットで解かれたことを明らかにしました。詳細は<a href="https://twitter.com/EpochAIResearch/status/1976685685349441826">@EpochAIResearch</a>のスレッドおよび<a href="https://twitter.com/EpochAIResearch/status/1976685757369851990">保持セットの詳細</a>、<a href="https://twitter.com/EpochAIResearch/status/1976685769130705300">過去の総計</a>を参照ください。Gemini 2.5 Deep Thinkの好成績については<a href="https://twitter.com/YiTayML/status/1976470535308734575">@YiTayML</a>や<a href="https://twitter.com/_philschmid/status/1976626257090535432">@_philschmid</a>も言及しています。FrontierMathサイトはこちら<a href="https://twitter.com/EpochAIResearch/status/1976685780862144978">@EpochAIResearch</a>。</li>
<li>Markovian Thinking（Delethink）：MilaとMicrosoftが、モデルに固定境界で「状態を書き込む」訓練を提案。推論の長さをコンテキストサイズから切り離し、推論を線形計算に変換します。R1‑Distill 1.5Bモデルは8Kコンテキストで24Kトークンまで推論し、フル24Kで訓練されたLongCoT‑RLを約4倍少ない計算（7対27 H100‑months）で上回りました。<a href="https://twitter.com/jiqizhixin/status/1976466786565656986">@jiqizhixin</a>や<a href="https://twitter.com/TheTuringPost/status/1976798665038758377">@TheTuringPost</a>による概要、<a href="https://twitter.com/TheTuringPost/status/1976798717094379588">効率の詳細</a>、<a href="https://twitter.com/TheTuringPost/status/1976798729274544403">論文/コード</a>も参照ください。</li>
<li>推論トレーニングが実際に教えること：新たな研究では、ベースモデルはすでに推論メカニズムを持っており、「思考モデル」はそれを呼び出すタイミングを学ぶと主張。適切なタイミングでスキルを呼び出すことで、ベースモデルと推論モデルの間の差の最大91%を回復できるとしています。<a href="https://twitter.com/cvenhoff00/status/1976633766811734461">@cvenhoff00</a>のスレッドや<a href="https://twitter.com/NeelNanda5/status/1976660983084130377">@NeelNanda5</a>のコメント、<a href="https://twitter.com/NeelNanda5/status/1976710233692012619">フォローアップ</a>を参照ください。</li>
<li>数学に対するRLの一般化への注意：いくつかの結果は、すでに数学に特化して中間訓練されたQwenベースを使用しており、この設定から広範な主張を外挿する際には注意が必要です（<a href="https://twitter.com/lateinteraction/status/1976761442842849598">@lateinteraction</a>）。</li>
</ul>
<p><strong>システムと推論：Blackwell + vLLM、適応型スペキュレーター、スパースアテンションKV階層化</strong></p>
<ul>
<li>NVIDIA Blackwell + vLLMがInferenceMAXで優勝：vLLMはNVIDIAとの深い共同作業により強力なパレート改善を示しました。スタック全体で100以上のPR、FP4/FP8カーネル、非同期スケジューリング、グラフ融合、FlashInfer統合などを実施。さらに推測デコードやData + Expert Parallel（DEP）によって2〜3倍のスループット向上が見込まれます。<a href="https://twitter.com/mgoin_/status/1976452383258648972">@mgoin_</a>、<a href="https://twitter.com/NVIDIAAIDev/status/1976686560398426456">@NVIDIAAIDev</a>の要約や<a href="https://twitter.com/SemiAnalysis_/status/1976669740035977702">ベンチマーク配信</a>をご覧ください。</li>
<li>ATLAS（Together AI）：ライブトラフィックから学習する適応型推測デコードシステム。DeepSeek‑V3.1でベースライン比4倍高速（500 TPS）を達成し、使用とともに改善。<a href="https://twitter.com/togethercompute/status/1976655646474031362">@togethercompute</a>のスレッドや<a href="https://twitter.com/togethercompute/status/1976655647925215339">適応型解説</a>、<a href="https://twitter.com/togethercompute/status/1976655649120612525">結果</a>、<a href="https://twitter.com/tri_dao/status/1976692444977938499">@tri_dao</a>を参照ください。初期報告では自己適応型スペキュレーターによりRL訓練時間が60%以上短縮（<a href="https://twitter.com/BlackHC/status/1976730114902851908">@BlackHC</a>）、VBでの報道はこちら<a href="https://twitter.com/togethercompute/status/1976743626685530540">リンク</a>。</li>
<li>Dynamic Sparse Attention向けSparseServe：DSAではKVキャッシュの常駐によりボトルネックがHBM帯域幅からHBM容量に移ります。SparseServeはHBM↔DRAM KV階層化（GPU FlashH2D、CPU FlashD2H）、作業セット認識型動的バッチ処理、レイヤー分割プレフィルを導入し、vLLMベースのテストでTTFTを9.26倍低減、スループットを3.14倍向上。<a href="https://twitter.com/ZhihuFrontier/status/1976544233700929614">@ZhihuFrontier</a>による概要や<a href="https://twitter.com/teortaxesTex/status/1976556643031933352">@teortaxesTex</a>によるハードウェア的示唆も参照ください。</li>
<li>カーネル速度 &gt; 「汎用ハードウェア」：Tritonが障壁を下げ、Blackwell速度では高レベルのオーバーヘッドが支配的になるため、MoE、低精度matmul、アテンション変種、SSMなどのカスタムカーネルが増える見込み（<a href="https://twitter.com/awnihannun/status/1976715815019037101">@awnihannun</a>）。</li>
</ul>
<p><strong>モデルとツールのリリース</strong></p>
<ul>
<li>Qwen3‑VL Cookbooks：ローカル/API利用向けのマルチモーダルタスク用ノートブック集。コンピュータ操作、全方位認識、文書解析/OCR、3Dグラウンディング、動画理解、モバイルエージェント、長文理解、空間推論などを含みます。<a href="https://twitter.com/Alibaba_Qwen/status/1976479304814145877">@Alibaba_Qwen</a>の投稿内リンクを参照ください。</li>
<li>音声から音声へ：GPT Realtime Mini（OpenAI）はフラッグシップRealtimeの約7分の1のコストで、TTFAを1.27秒から0.81秒に短縮、コンテキストを32kに倍増し、画像入力を追加。WebRTC/WebSocket/SIP経由でのスケーラブルなエージェント向けに位置付けられています。Gemini 2.5 Flash Native Audio Dialogとの比較分析は<a href="https://twitter.com/ArtificialAnlys/status/1976696262083985636">@ArtificialAnlys</a>の<a href="https://twitter.com/ArtificialAnlys/status/1976696264080474365">チャート</a>、<a href="https://twitter.com/ArtificialAnlys/status/1976696265754001747">モデル探索</a>をご覧ください。</li>
<li>小型・高速・オープンなビジョン：Moondream 3（9B、64エキスパートMoE、約2Bアクティブ）はネイティブポインティング、OCR改善、32Kコンテキストを追加。UI理解やエージェントワークフローに最適化。<a href="https://twitter.com/moondreamai/status/1976624914070401142">@moondreamai</a>の発表やFALでのプレビュー<a href="https://twitter.com/fal/status/1976682702167228919">@fal</a>を参照ください。</li>
<li>エージェント型コーディング：KAT‑Dev‑72B‑Exp（Kwaipilot）がSWE‑Bench Verifiedで2位。中間訓練→SFT+RFT→エージェント型RLで調整。4×RTX 3090 @ 4‑bitで動作（<a href="https://twitter.com/TheAhmadOsman/status/1976606921756205531">@TheAhmadOsman</a>）。</li>
<li>LoRA/QLoRA/DoRA/QDoRAによるRL後訓練：Tora（torchtuneベース）はGRPO、FSDP、コンパイル対応を統合。安定した4‑bit RL（QLoRA/QDoRA）を可能にし、DoRA‑Cacheでロールアウトを2〜4倍高速化（<a href="https://twitter.com/gm8xx8/status/1976443792850092464">@gm8xx8</a>）。</li>
<li>ツール関連速報：LangSmithがPythonに加えJSコード評価をサポート（<a href="https://twitter.com/LangChainAI/status/1976700402105233603">@LangChainAI</a>）、LangChain v1がカスタマイズ可能なcreate_agentやモデル/ツール呼び出し前後のミドルウェアフックを提供（<a href="https://twitter.com/sydneyrunkle/status/1976751776620593564">@sydneyrunkle</a>、<a href="https://twitter.com/sydneyrunkle/status/1976753314462417344">フック解説</a>）、LlamaIndexがカスタムルールによる説明可能な文書分類を追加（<a href="https://twitter.com/llama_index/status/1976686683468026337">@llama_index</a>）、Glass HealthがHIPAA準拠と引用メタデータ付きの本番開発者APIを開始（<a href="https://twitter.com/GlassHealthHQ/status/1976713436773138599">@GlassHealthHQ</a>）。</li>
</ul>
<p><strong>スケール・計算・訓練推定</strong></p>
<ul>
<li>月間処理トークン数：Google約1.3京、OpenAI約260兆、Groq約50兆（<a href="https://twitter.com/sundeep/status/1976475987962626062">@sundeep</a>）。GoogleのDemis Hassabisも1.3京トークン/月を再確認（<a href="https://twitter.com/demishassabis/status/1976712484657475691">@demishassabis</a>）。トークンはモデル/語彙/タスクによって情報密度や有用性が異なる（<a href="https://twitter.com/awnihannun/status/1976676812022550864">@awnihannun</a>）。</li>
<li>計算資源の使途：EpochはOpenAIが昨年約70億ドルを計算に費やし、その大半がR&amp;D（実験や失敗ラン）で、最終訓練ランは10億ドル未満と推定（<a href="https://twitter.com/EpochAIResearch/status/1976714284349767990">@EpochAIResearch</a>、<a href="https://twitter.com/EpochAIResearch/status/1976714297255588053">フォローアップ</a>）。</li>
<li>GPT‑5訓練の推測：外部推定では約1000億アクティブパラメータ、30〜100兆トークン、RLが事前学習の10〜100%、総計約6e25 FLOPs（<a href="https://twitter.com/teortaxesTex/status/1976441366969532888">@teortaxesTex</a>）。MoEのスパース性に関する議論では、非常に多い総パラメータ数と小さなアクティブサブセット（例：256〜1024エキスパートのうち4〜8アクティブ）が示唆されますが、実際のアクティブ数やコストは見出しより低い可能性もあります（<a href="https://twitter.com/teortaxesTex/status/1976773126584516801">分析スレッド</a>）。</li>
</ul>
<p><strong>ロボティクスと実体化AI</strong></p>
<ul>
<li>ハードウェアでのアクロバット再ターゲット：OmniRetarget + BeyondMimicの最小RLトラッキングを使用し、ヒューマノイドが壁宙返りを5/5成功率で実行。訓練には終了条件の緩和や報酬調整などの軽微な変更のみが必要（<a href="https://twitter.com/zhenkirito123/status/1976663920552427619">@zhenkirito123</a>）。別件ではUnitree G1がテコンドーの回し蹴りを再現し、IMUジャイロ飽和の問題を調整で解決（<a href="https://twitter.com/kevin_zakka/status/1976460408077812085">@kevin_zakka</a>）。</li>
<li>エージェント向けビジョン：Moondream 3は現実世界のUIや構造化知覚を下流のエージェントフレームワーク向けにターゲット（モデルリリース参照）。</li>
</ul>
<p><strong>評価・ベンチマーク・ガバナンス</strong></p>
<ul>
<li>ベンチマーク改革：「Benchmarking is Broken—Don’t Let AI be its Own Judge」ではPeerBenchを提案。コミュニティ主導・監督付き評価の設計図として、封印実行、ローリング問題バンク、透明性の遅延を導入（<a href="https://twitter.com/iScienceLuvr/status/1976586775603851344">@iScienceLuvr</a>）。</li>
<li>CoT透明性：研究所はChain-of-Thoughtに対して訓練しているかどうかを開示すべきと<a href="https://twitter.com/RyanPGreenblatt/status/1976686565654221150">@RyanPGreenblatt</a>が主張。METRのGPT‑5評価を引用し、知的財産が敏感な場合は信頼できる評価者への第三者開示を提案（<a href="https://twitter.com/RyanPGreenblatt/status/1976686574080491880">フォローアップ</a>）。</li>
<li>OpenBench拡張：GroqのOpenBenchがARC‑AGIをサポートし、推論ベンチマークの標準化評価範囲を拡大（<a href="https://twitter.com/GregKamradt/status/1976718318544601573">@GregKamradt</a>）。</li>
<li>「野生での評価」とゴールポストの移動：静的テストセットを超えた実践的評価が重視される傾向（<a href="https://twitter.com/lateinteraction/status/1976439833158615345">@lateinteraction</a>）。おもちゃテストから持続的自律性や経済的影響への文化的シフトを<a href="https://twitter.com/aidan_mclau/status/1976658416451149874">@aidan_mclau</a>が要約。</li>
<li>ガバナンス論争（文脈のみ）：OpenAI召喚状に関するEncode GCスレッドが内外で議論を呼び、<a href="https://twitter.com/_NathanCalvin/status/1976649051396620514">@_NathanCalvin</a>、OpenAIの<a href="https://twitter.com/jasonkwon/status/1976762546041634878">@jasonkwon</a>、<a href="https://twitter.com/jachiam0/status/1976690339546112098">@jachiam0</a>による批判的懸念が示されました。政策議論や公開性の規範への影響に注目。</li>
</ul>
<p><strong>エンゲージメント上位ツイート</strong></p>
<ul>
<li>「エンジニアにインタビューしたら…99% AIヘルパーを使っていた」—採用シグナルの変化と評価の衛生（<a href="https://twitter.com/Yuchenj_UW/status/1976700504152719435">@Yuchenj_UW</a>）。</li>
<li>OmniRetarget + BeyondMimicによる驚異的な壁宙返り（<a href="https://twitter.com/zhenkirito123/status/1976663920552427619">@zhenkirito123</a>）。</li>
<li>Demis：Googleが1か月で約1.3京トークンを処理（<a href="https://twitter.com/demishassabis/status/1976712484657475691">@demishassabis</a>）。</li>
<li>「2024年の評価 vs 2025年の評価」ミーム—長期的影響重視へのシフト（<a href="https://twitter.com/aidan_mclau/status/1976658416451149874">@aidan_mclau</a>）。</li>
<li>政策議論を促した召喚状スレッド（<a href="https://twitter.com/_NathanCalvin/status/1976649051396620514">@_NathanCalvin</a>）。</li>
</ul>
<hr />
<h1 id="ai-reddit">AI Redditまとめ</h1>
<h2 id="rlocalllama-rlocalllm">/r/LocalLlama + /r/localLLMまとめ</h2>
<p>該当なし</p>
<h2 id="ai-subreddit">技術的でないAI Subredditまとめ</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3 id="1-nvidia-gb300-nvl72-comfyui-gds">1. NVIDIA GB300 NVL72 + ComfyUI GDS性能アップデート</h3>
<ul>
<li><a href="https://www.reddit.com/r/singularity/comments/1o2t53m/microsoft_unveils_the_first_atscale_nvidia_gb300/"><strong>Microsoft、初の大規模NVIDIA GB300 NVL72クラスターを発表。OpenAIが数兆パラメータモデルを数日で訓練可能に</strong></a>：Microsoft/AzureがOpenAI向けに初の本番NVIDIA GB300 NVL72クラスター（NDv6 GB300）を発表。4,600台以上のBlackwell Ultra GPUを搭載し、各NVL72 VMはNVLink Switchファブリック（ラックあたり130 TB/s）で72 GPUを統合し、37 TBのアクセラレータを形成。VMあたり<code>1.44エクサFLOPS FP4</code>を提供し、ラック間はQuantum‑X800 InfiniBandで<code>GPUあたり800 Gb/s</code>接続。低精度訓練/推論向けにNVFP4やNVIDIA Dynamoコンパイラ、SHARP v4、ConnectX‑8 SuperNICを採用。AzureはMLPerf Inference v5.1でのリーダーシップを強調（例：<code>671B</code>推論モデルでHopper比最大<code>5×</code>スループット）。概算では64 NVL72 VMで総計約<code>92エクサFLOPS FP4</code>ピーク。FP4指標はFP64 TOP500/HPLシステムとは直接比較不可。コメントではシステム間比較のため精度、GPUあたりFLOPS、バンド幅、NIC速度、MLPerf結果の正規化を提案。MoEによるパラメータスケーリングと、Chinchilla最適な密モデルのトークン要求（数十兆）について議論。<ul>
<li>NVL72はラックスケールの「島」で、NVSwitchで全GPUが単一NVLinkドメインを共有。複数島はInfiniBand/Ethernetで接続。大規模モデル訓練では島内通信が高速で重要。</li>
<li>他スーパーコンピュータとの比較には、GPUあたりAI FLOPSとHBM容量、島内バンド幅、MLPerf訓練時間が有効。</li>
<li>高品質テキスト+コードコーパスは約10〜30兆トークンと推定。Chinchilla則では1兆パラメータ密モデルに約20兆トークンが必要。MoEではアクティブパラメータを抑えつつ総パラメータを拡張可能。</li>
</ul>
</li>
<li><a href="https://www.reddit.com/r/StableDiffusion/comments/1o2wklr/we_can_now_run_wan_or_any_heavy_models_even_on_a/"><strong>6GB NVIDIAノートGPUでも重いモデルを実行可能に—ComfyUIへのGDS統合</strong></a>：開発者MaifeeがNVIDIA GPUDirect Storage（GDS）をComfyUIに統合し、NVMeからGPU VRAMへの直接ストリーミングを実現。VRAM 6GBでも重いモデルを実行可能に。Linux、対応ドライバ、CUDA、ext4/xfsが必要。コメントではRAMオフロードとの違い（CPU経由を避けるゼロコピーDMA）やLinux限定である点が議論。</li>
</ul>
<h3 id="2-anisora-v32wan22360-i2vsora-2">2. AniSora V3.2（Wan2.2）360° I2VとSora-2デモ</h3>
<ul>
<li><a href="https://www.reddit.com/r/StableDiffusion/comments/1o2qjiw/360_anime_spins_with_anisora_v32/"><strong>AniSora V3.2による360°アニメ回転</strong></a>：Wan2.2 I2Vベースのアニメ特化モデル。ComfyUIワークフローに直接組み込み、入力イラストから滑らかな360°回転動画を生成。コメントでは3D資産パイプラインへの応用や非写実スタイルへの汎化が議論。</li>
<li><a href="https://www.reddit.com/r/singularity/comments/1o36ptd/hyperspace_and_beyond/"><strong>Hyperspace and Beyond</strong></a>：非技術的ミーム。研究論文スレッドにGIFだけを貼る行為を風刺。</li>
<li><a href="https://www.reddit.com/r/singularity/comments/1o2u46w/figure_doing_housework_barely_honestly_would_be/"><strong>Figureロボットの家事</strong></a>：Figureヒューマノイドが家庭清掃を試みる映像。夜間自動運転の安全性懸念も。</li>
<li><a href="https://www.reddit.com/r/ChatGPT/comments/1o2onjk/this_is_cheating_at_this_point/"><strong>これはもう反則では？😂</strong></a>：OpenAI Soraによるリアルな映像生成。一般視聴者が本物と誤認するリスクが指摘。</li>
<li><a href="https://www.reddit.com/r/ChatGPT/comments/1o2w0fm/im_sorry_but_this_is_some_of_the_funniest_al_ive/"><strong>最も面白いAI音声</strong></a>：AI音声合成によるジョーク。音声のみAI生成で映像は実写。</li>
</ul>
<h3 id="3-doordashamazon">3. 配達失敗：DoorDashポーチ崩壊とAmazon配達</h3>
<ul>
<li><a href="https://www.reddit.com/r/aivideo/comments/1o2qdyg/600lb_doordasher_falls_through_the_porch_floor/"><strong>DoorDash配達員がポーチ床を突き破る</strong></a>：映像がAI生成か実写か議論。物理表現の精度が話題。</li>
<li><a href="https://www.reddit.com/r/aivideo/comments/1o2p45q/amazon_delivery/"><strong>Amazon配達</strong></a>：犬が窓から飛び出し茂みに衝突する映像。葉の変形や動きのリアルさが評価される一方、細部の物理的リアリズムには課題。</li>
</ul>
<hr />
<h1 id="ai-discord">AI Discordまとめ</h1>
<blockquote>
<p>gpt-5による要約</p>
</blockquote>
<p><strong>1. 推論高速化とカーネル最適化</strong></p>
<ul>
<li><strong>Predicted-Outputs PrefillによるvLLM高速化</strong>：CascadeがvLLM向けに予測出力をプレフィルとして活用する機能を発表。部分一致の予測をキャッシュ計算に変換し、API変更なしで遅延を削減。</li>
<li><strong>Residual再計算によるスループット向上</strong>：LLMQがアテンション残差の再計算を実装し、メモリ負荷を軽減。Qwen2.5-14BでTPSが大幅向上。</li>
<li><strong>Swizzlingとldmatrixの最適化</strong>：Tritonのldmatrixタイル計算ロジックや最適スウィズル方法が共有され、PTXドキュメントとの不一致やコンパイラエラーが議論。</li>
</ul>
<p><strong>2. 小型モデルのベンチマーク</strong></p>
<ul>
<li><strong>Samsungの小型Recursive ModelがARCで好成績</strong>：TRMがARC-AGI-1で44.6%を達成し、大型モデルを上回る。</li>
<li><strong>ARCベンチマーク過学習懸念</strong>：小規模公開セットへのデータ拡張による過学習の可能性が指摘。</li>
<li><strong>ハイパーパラメータ調整で拡散ステップ削減</strong>：8ステップで20ステップ同等のFIDを達成し、計算を60%削減。</li>
</ul>
<p><strong>3. AI資金調達・M&amp;A</strong></p>
<ul>
<li><strong>SpellbookがシリーズBで5,000万ドル調達</strong>：契約書向けAI支援ツール。</li>
<li><strong>Datacurveが1,770万ドル調達</strong>：高品質訓練データ提供。</li>
<li><strong>ElasticがJina AIを買収</strong>：検索・エージェント機能強化。</li>
</ul>
<p><strong>4. プロトコル標準と構造化ツール</strong></p>
<ul>
<li><strong>MCPメタデータ用.well-known提案</strong>：サーバー識別と発見を容易に。</li>
<li><strong>structuredContentでの画像表現議論</strong>：ポータビリティ問題と暫定的解決策。</li>
<li><strong>skybridgeツールのschema未使用問題</strong>：柔軟性重視の方向性。</li>
</ul>
<hr />
<h1 id="discord">Discord: ハイレベルまとめ</h1>
<h2 id="perplexity-ai-discord">Perplexity AI Discord</h2>
<ul>
<li><strong>GPT-5のコーディング評価</strong>：GPT-5とClaude 4.5の比較議論。</li>
<li><strong>Perplexity Proの制限</strong>：検索や動画生成の月間上限。</li>
<li><strong>Comet Browserモバイル版待望</strong>。</li>
<li><strong>GPTsエージェントの知識更新不可</strong>。</li>
<li><strong>Search APIがCloudflareにより拒否される問題</strong>。</li>
</ul>
<h2 id="lmarena-discord">LMArena Discord</h2>
<ul>
<li><strong>料金プラン議論</strong>：無料維持の意向。</li>
<li><strong>Sonnet 4.5のエラー頻発</strong>。</li>
<li><strong>Video Arenaの制限</strong>。</li>
<li><strong>Gemini 3.0の発売時期不明</strong>。</li>
<li><strong>Grokの動画生成機能評価</strong>。</li>
</ul>
<h2 id="openai-discord">OpenAI Discord</h2>
<ul>
<li><strong>LinuxCometのプライバシー懸念</strong>。</li>
<li><strong>GPT-5 Thinking Miniの自動切替問題</strong>。</li>
<li><strong>Samsung TRMのベンチマーク結果</strong>。</li>
<li><strong>ChatGPT Businessへのブランド変更</strong>。</li>
<li><strong>Sora 2のプロンプト設計議論</strong>。</li>
</ul>
<h2 id="openrouter-discord">OpenRouter Discord</h2>
<ul>
<li><strong>BYOK支払い問題</strong>。</li>
<li><strong>DeepSeekモデル制限への不満</strong>。</li>
<li><strong>SSE使用データ取得の不具合</strong>。</li>
<li><strong>Qwenの新モデル発表予定</strong>。</li>
</ul>
<h2 id="unsloth-ai-discord">Unsloth AI Discord</h2>
<ul>
<li><strong>Qwen3-VLのサポート方法</strong>。</li>
<li><strong>DGX Sparkの性能懸念</strong>。</li>
<li><strong>TRLライブラリのトレーナー削除計画</strong>。</li>
<li><strong>事前学習データセット探し</strong>。</li>
<li><strong>ファインチューニングの脆弱性議論</strong>。</li>
</ul>
<p>（以下、各Discordチャンネルの詳細は省略）</p>
<hr />
    </main>

    <footer>
        <p>
            <a href="https://github.com/YIPG/ainews">GitHub</a>
            <a href="https://news.smol.ai/">news.smol.ai</a>
        </p>
    </footer>
</body>
</html>